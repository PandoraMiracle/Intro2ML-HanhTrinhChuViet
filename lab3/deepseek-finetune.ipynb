{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"54ca4f86ebaf4f30b24876fd14217d0a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_169ef11619f541f4a50f08ab6dbb4d75","IPY_MODEL_48547cc4f5ea48b88533bf2e6a6dd8b8","IPY_MODEL_4c6a40dcad1645e6a2c0b8643576ba1e"],"layout":"IPY_MODEL_8d17d6845e6b494aaa06fa108986a0f4"}},"169ef11619f541f4a50f08ab6dbb4d75":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2bcc1ffe17ce4cefb9f8c21ec4c43491","placeholder":"​","style":"IPY_MODEL_eabb0ce1bc5f4401bc062cda7171ae7b","value":"Fetching 21 files: 100%"}},"48547cc4f5ea48b88533bf2e6a6dd8b8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e20992077ed49b880d1d52461fe5dfa","max":21,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4b748acb93b74f64967de117697d638a","value":21}},"4c6a40dcad1645e6a2c0b8643576ba1e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d154345b1b964cb293602e5223c6b4aa","placeholder":"​","style":"IPY_MODEL_2ca9991e95a949b3903be991090cd07c","value":" 21/21 [00:00&lt;00:00, 517.87it/s]"}},"8d17d6845e6b494aaa06fa108986a0f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2bcc1ffe17ce4cefb9f8c21ec4c43491":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eabb0ce1bc5f4401bc062cda7171ae7b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e20992077ed49b880d1d52461fe5dfa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b748acb93b74f64967de117697d638a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d154345b1b964cb293602e5223c6b4aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ca9991e95a949b3903be991090cd07c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dowload data","metadata":{"id":"dnTZpUl-3fnt"}},{"cell_type":"code","source":"!gdown 1BVjrYwkQ69SPXhABMpaWCwVie2OPlXL6\n!unzip -q data_word -d data","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cVayaK8srFdw","outputId":"c956c77b-7255-456b-aaf7-a3aee013086e","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T04:56:18.504318Z","iopub.execute_input":"2025-12-14T04:56:18.504895Z","iopub.status.idle":"2025-12-14T04:57:09.360514Z","shell.execute_reply.started":"2025-12-14T04:56:18.504871Z","shell.execute_reply":"2025-12-14T04:57:09.359544Z"}},"outputs":[{"name":"stdout","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=1BVjrYwkQ69SPXhABMpaWCwVie2OPlXL6\nFrom (redirected): https://drive.google.com/uc?id=1BVjrYwkQ69SPXhABMpaWCwVie2OPlXL6&confirm=t&uuid=9759606b-f0c7-4de4-9c7e-a961c2f79396\nTo: /kaggle/working/data_word.zip\n100%|███████████████████████████████████████| 1.46G/1.46G [00:13<00:00, 110MB/s]\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#!gdown 1JAbhGvyrNd8kkE9t92iZfLKppjyM1meU\n#!unzip -q checkpoint-800 -d checkpoint-800","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!gdown 1Xcz4lCqL8whIr6DB5kZB24RHyFaKciKC\n!gdown 1Z4RxmO7wytSMALvpl5qB8X4Utn_HJe18\n!unzip -q outputs_finetune -d outputs_finetune","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T04:57:09.362185Z","iopub.execute_input":"2025-12-14T04:57:09.362474Z","iopub.status.idle":"2025-12-14T04:57:46.862308Z","shell.execute_reply.started":"2025-12-14T04:57:09.362452Z","shell.execute_reply":"2025-12-14T04:57:46.861434Z"}},"outputs":[{"name":"stdout","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=1Xcz4lCqL8whIr6DB5kZB24RHyFaKciKC\nFrom (redirected): https://drive.google.com/uc?id=1Xcz4lCqL8whIr6DB5kZB24RHyFaKciKC&confirm=t&uuid=2fae6753-3161-492f-ac1c-9199c2bfb32c\nTo: /kaggle/working/outputs_finetune.zip\n100%|██████████████████████████████████████| 1.16G/1.16G [00:15<00:00, 76.8MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1Z4RxmO7wytSMALvpl5qB8X4Utn_HJe18\nTo: /kaggle/working/split.json\n100%|██████████████████████████████████████| 15.1M/15.1M [00:00<00:00, 20.6MB/s]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nimport torch\nprint(torch.cuda.device_count())\nprint(torch.cuda.get_device_name(0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T04:57:46.863460Z","iopub.execute_input":"2025-12-14T04:57:46.864129Z","iopub.status.idle":"2025-12-14T04:57:51.475865Z","shell.execute_reply.started":"2025-12-14T04:57:46.864100Z","shell.execute_reply":"2025-12-14T04:57:51.474963Z"}},"outputs":[{"name":"stdout","text":"1\nTesla T4\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Installation","metadata":{"id":"usawnG4l3ilN"}},{"cell_type":"code","source":"%%capture\nimport os, re\nif \"COLAB_\" not in \"\".join(os.environ.keys()):\n    !pip install unsloth\nelse:\n    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n    !pip install --no-deps unsloth\n!pip install transformers==4.56.2\n!pip install --no-deps trl==0.22.2\n!pip install jiwer\n!pip install einops addict easydict\n","metadata":{"id":"Jbx2HeqvwOAe","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T04:57:51.478300Z","iopub.execute_input":"2025-12-14T04:57:51.478930Z","iopub.status.idle":"2025-12-14T04:58:41.326220Z","shell.execute_reply.started":"2025-12-14T04:57:51.478901Z","shell.execute_reply":"2025-12-14T04:58:41.325254Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"!pip install transformers==4.56.2\n!pip install --no-deps trl==0.22.2\n!pip install jiwer\n!pip install einops addict easydict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T04:58:41.327400Z","iopub.execute_input":"2025-12-14T04:58:41.327751Z","iopub.status.idle":"2025-12-14T04:58:53.757509Z","shell.execute_reply.started":"2025-12-14T04:58:41.327723Z","shell.execute_reply":"2025-12-14T04:58:53.756623Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers==4.56.2 in /usr/local/lib/python3.11/dist-packages (4.56.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.2) (3.20.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.2) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.2) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.2) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.2) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.2) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.2) (2.32.5)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.2) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.2) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.2) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2) (2025.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.56.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.56.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.56.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.56.2) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.56.2) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.56.2) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.56.2) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.56.2) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.56.2) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.56.2) (2025.10.5)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.56.2) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.56.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.56.2) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.56.2) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.56.2) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.56.2) (2024.2.0)\nRequirement already satisfied: trl==0.22.2 in /usr/local/lib/python3.11/dist-packages (0.22.2)\nRequirement already satisfied: jiwer in /usr/local/lib/python3.11/dist-packages (4.0.0)\nRequirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.3.0)\nRequirement already satisfied: rapidfuzz>=3.9.7 in /usr/local/lib/python3.11/dist-packages (from jiwer) (3.14.3)\nRequirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\nRequirement already satisfied: addict in /usr/local/lib/python3.11/dist-packages (2.4.0)\nRequirement already satisfied: easydict in /usr/local/lib/python3.11/dist-packages (1.13)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Unsloth","metadata":{"id":"yI58SfnL3nqd"}},{"cell_type":"code","source":"#!pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo transformers timm\n\nfrom huggingface_hub import snapshot_download\nsnapshot_download(\"unsloth/DeepSeek-OCR\", local_dir = \"deepseek_ocr\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":834,"referenced_widgets":["54ca4f86ebaf4f30b24876fd14217d0a","169ef11619f541f4a50f08ab6dbb4d75","48547cc4f5ea48b88533bf2e6a6dd8b8","4c6a40dcad1645e6a2c0b8643576ba1e","8d17d6845e6b494aaa06fa108986a0f4","2bcc1ffe17ce4cefb9f8c21ec4c43491","eabb0ce1bc5f4401bc062cda7171ae7b","0e20992077ed49b880d1d52461fe5dfa","4b748acb93b74f64967de117697d638a","d154345b1b964cb293602e5223c6b4aa","2ca9991e95a949b3903be991090cd07c"]},"id":"KwbsPtwIxZWP","outputId":"064c39dd-deea-4dc8-a595-11985c1cdc32","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T04:58:53.758738Z","iopub.execute_input":"2025-12-14T04:58:53.759033Z","iopub.status.idle":"2025-12-14T04:59:13.893324Z","shell.execute_reply.started":"2025-12-14T04:58:53.759006Z","shell.execute_reply":"2025-12-14T04:59:13.892593Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Fetching 21 files:   0%|          | 0/21 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e59e7e509dfb4c86b65c267432aa5a4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"318cc59eee414abaad29568cee8c6905"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README-checkpoint.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"085b7df33cdd430da19565b4b2c159cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16ed10511d6e48d3b223c17d793227f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"assets/show1.jpg:   0%|          | 0.00/117k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4def6895121d441dbdc3c0ba00cc4321"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"assets/show2.jpg:   0%|          | 0.00/216k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de123eae47b640c0a88612808f4223de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"assets/fig1.png:   0%|          | 0.00/396k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ce6c55d5bc44be9b6c48dfa974afb24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"LICENSE: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9706d03241f426489e5583838d5f854"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"assets/show3.jpg:   0%|          | 0.00/247k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4ca1cdecb524ba8b540d005be3cfb24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c986a105b8ef483cb712a8d4e462e010"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"conversation.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ee9e14d13214096b82cc6c656a0ff85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_deepseek_v2.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff801ddbbaa04fb8a4346ad7d8854c8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"assets/show4.jpg:   0%|          | 0.00/269k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85573a9a4b304beea529881a844af47c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"deepencoder.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2f9920cea53449ab37d94cc76e62f9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b803fa56dcb94ea6bffe38a709f7a631"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-000001.safetensors:   0%|          | 0.00/6.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f912d45fb3c54e54a9721e3284494b35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modeling_deepseekocr.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8436b0cd9b7f4ec6bcf6aa6c8a76c40c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modeling_deepseekv2.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b25d2aaf084c4e31898fc3b68b4bbedb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"processor_config.json:   0%|          | 0.00/460 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cae073eb6004d1d9d1d655bd86ce6a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/801 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffa572f708f54bbcabf07075607e4597"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90212f3bc60d4c47bb9a22a200a5fcea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64edd74b95184559833b505262bfbe80"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/deepseek_ocr'"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import os, logging\nos.environ[\"UNSLOTH_WARN_UNINITIALIZED\"] = \"0\"\n\n# Gỡ các handler dạng Unsloth khỏi logger transformers.modeling_utils\nlogger = logging.getLogger(\"transformers.modeling_utils\")\nnew_handlers = []\nfor h in logger.handlers:\n    # Unsloth thường đặt handler có class name đặc trưng, ta lọc theo tên\n    if \"Unsloth\" in h.__class__.__name__:\n        continue\n    new_handlers.append(h)\nlogger.handlers = new_handlers\n","metadata":{"id":"G5DV_9_h3G9k","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T04:59:13.894446Z","iopub.execute_input":"2025-12-14T04:59:13.895884Z","iopub.status.idle":"2025-12-14T04:59:13.903652Z","shell.execute_reply.started":"2025-12-14T04:59:13.895848Z","shell.execute_reply":"2025-12-14T04:59:13.902607Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Import","metadata":{"id":"8EHBWnuSV7d9"}},{"cell_type":"code","source":"import os, json, random, argparse, unicodedata\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Any, Tuple\n\nimport torch\nfrom jiwer import wer, cer\n#from huggingface_hub import snapshot_download\n\n#from unsloth import FastVisionModel, is_bf16_supported\nfrom transformers import AutoModel, Trainer, TrainingArguments","metadata":{"id":"b5VLXfUWV84G","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T04:59:13.904991Z","iopub.execute_input":"2025-12-14T04:59:13.905339Z","iopub.status.idle":"2025-12-14T05:00:05.006089Z","shell.execute_reply.started":"2025-12-14T04:59:13.905317Z","shell.execute_reply":"2025-12-14T05:00:05.005220Z"}},"outputs":[{"name":"stderr","text":"2025-12-14 04:59:40.762176: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765688380.974449      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765688381.036229      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"print(torch.cuda.device_count())\nprint(torch.cuda.get_device_name(0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T05:00:42.061217Z","iopub.execute_input":"2025-12-14T05:00:42.061758Z","iopub.status.idle":"2025-12-14T05:00:42.066882Z","shell.execute_reply.started":"2025-12-14T05:00:42.061730Z","shell.execute_reply":"2025-12-14T05:00:42.065974Z"}},"outputs":[{"name":"stdout","text":"1\nTesla T4\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Fine-Tuning","metadata":{"id":"ZpwFkK_BWcqy"}},{"cell_type":"code","source":"# ========= (1) Text utils =========\ndef normalize_text(s: str) -> str:\n    # Giữ dấu tiếng Việt ổn định (NFC), normalize whitespace nhẹ\n    s = unicodedata.normalize(\"NFC\", s or \"\")\n    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n    s = \" \".join(s.split())\n    return s.strip()\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)","metadata":{"id":"tcYFagGoWhVu","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T05:00:47.717693Z","iopub.execute_input":"2025-12-14T05:00:47.717989Z","iopub.status.idle":"2025-12-14T05:00:47.723066Z","shell.execute_reply.started":"2025-12-14T05:00:47.717966Z","shell.execute_reply":"2025-12-14T05:00:47.722339Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Data Prep","metadata":{"id":"aKE43nWjWtyN"}},{"cell_type":"code","source":"# ========= (2) Data loader: scan .png/.txt pairs =========\ndef collect_pairs(root: str, recursive: bool = True) -> List[Dict[str, str]]:\n    \"\"\"\n    Expect:\n      <stem>.png\n      <stem>.txt   (ground truth)\n    Return list of {\"image_path\": ..., \"text\": ...}\n    \"\"\"\n    items = []\n    walker = os.walk(root) if recursive else [(root, [], os.listdir(root))]\n    for d, _, files in walker:\n        pngs = [f for f in files if f.lower().endswith(\".png\")]\n        for png in pngs:\n            stem = png[:-4]\n            txt = stem + \".txt\"\n            img_path = os.path.join(d, png)\n            txt_path = os.path.join(d, txt)\n            if not os.path.exists(txt_path):\n                continue\n            with open(txt_path, \"r\", encoding=\"utf-8-sig\") as f:\n                gt = normalize_text(f.read())\n            if gt == \"\":\n                continue\n            items.append({\"image_path\": img_path, \"text\": gt})\n    return items\n\n# ========= (3) Conversation format =========\ndef to_conversation(sample: Dict[str, str], instruction: str) -> Dict[str, Any]:\n    # NOTE: để nhẹ RAM, ta truyền image_path (string) và collator sẽ tự Image.open\n    return {\n        \"messages\": [\n            {\"role\": \"<|User|>\", \"content\": instruction, \"images\": [sample[\"image_path\"]]},\n            {\"role\": \"<|Assistant|>\", \"content\": sample[\"text\"]},\n        ]\n    }\n","metadata":{"id":"H9GcPNPRW6hM","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T05:00:50.017743Z","iopub.execute_input":"2025-12-14T05:00:50.018410Z","iopub.status.idle":"2025-12-14T05:00:50.026971Z","shell.execute_reply.started":"2025-12-14T05:00:50.018380Z","shell.execute_reply":"2025-12-14T05:00:50.025839Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Create datacollator","metadata":{"id":"IwWQaX4CW9kQ"}},{"cell_type":"code","source":"# ========= (4) Collator (dựa trên class bạn đang dùng, thêm hỗ trợ path string) =========\nimport math, io\nfrom PIL import Image, ImageOps\nfrom torch.nn.utils.rnn import pad_sequence\nfrom deepseek_ocr.modeling_deepseekocr import text_encode, BasicImageTransform, dynamic_preprocess\n\nclass DeepSeekOCRDataCollator:\n    def __init__(self, tokenizer, model, image_size=640, base_size=1024, crop_mode=True, train_on_responses_only=True):\n        self.tokenizer = tokenizer\n        self.model = model\n        self.image_size = image_size\n        self.base_size = base_size\n        self.crop_mode = crop_mode\n        self.image_token_id = 128815\n        self.dtype = model.dtype\n        self.train_on_responses_only = train_on_responses_only\n\n        self.image_transform = BasicImageTransform(\n            mean=(0.5, 0.5, 0.5),\n            std=(0.5, 0.5, 0.5),\n            normalize=True\n        )\n        self.patch_size = 16\n        self.downsample_ratio = 4\n\n        self.bos_id = tokenizer.bos_token_id if getattr(tokenizer, \"bos_token_id\", None) is not None else 0\n\n    def deserialize_image(self, image_data) -> Image.Image:\n        # ====== PATCH quan trọng cho dataset của bạn: support path string ======\n        if isinstance(image_data, str):\n            return Image.open(image_data).convert(\"RGB\")\n        if isinstance(image_data, Image.Image):\n            return image_data.convert(\"RGB\")\n        if isinstance(image_data, dict):\n            if \"bytes\" in image_data:\n                image = Image.open(io.BytesIO(image_data[\"bytes\"]))\n                return image.convert(\"RGB\")\n            if \"path\" in image_data:\n                return Image.open(image_data[\"path\"]).convert(\"RGB\")\n        raise ValueError(f\"Unsupported image format: {type(image_data)}\")\n\n    def process_image(self, image: Image.Image):\n        images_list, images_crop_list, images_spatial_crop = [], [], []\n\n        if self.crop_mode:\n            if image.size[0] <= 640 and image.size[1] <= 640:\n                crop_ratio = (1, 1)\n                images_crop_raw = []\n            else:\n                images_crop_raw, crop_ratio = dynamic_preprocess(\n                    image, min_num=2, max_num=9, image_size=self.image_size, use_thumbnail=False\n                )\n\n            global_view = ImageOps.pad(\n                image, (self.base_size, self.base_size),\n                color=tuple(int(x * 255) for x in self.image_transform.mean)\n            )\n            images_list.append(self.image_transform(global_view).to(self.dtype))\n\n            width_crop_num, height_crop_num = crop_ratio\n            images_spatial_crop.append([width_crop_num, height_crop_num])\n\n            if width_crop_num > 1 or height_crop_num > 1:\n                for crop_img in images_crop_raw:\n                    images_crop_list.append(self.image_transform(crop_img).to(self.dtype))\n\n            num_queries = math.ceil((self.image_size // self.patch_size) / self.downsample_ratio)\n            num_queries_base = math.ceil((self.base_size // self.patch_size) / self.downsample_ratio)\n\n            tokenized_image = ([self.image_token_id] * num_queries_base + [self.image_token_id]) * num_queries_base\n            tokenized_image += [self.image_token_id]\n            if width_crop_num > 1 or height_crop_num > 1:\n                tokenized_image += ([self.image_token_id] * (num_queries * width_crop_num) + [self.image_token_id]) * (\n                    num_queries * height_crop_num\n                )\n        else:\n            crop_ratio = (1, 1)\n            images_spatial_crop.append([1, 1])\n\n            global_view = ImageOps.pad(\n                image, (self.base_size, self.base_size),\n                color=tuple(int(x * 255) for x in self.image_transform.mean)\n            )\n            images_list.append(self.image_transform(global_view).to(self.dtype))\n\n            num_queries = math.ceil((self.base_size // self.patch_size) / self.downsample_ratio)\n            tokenized_image = ([self.image_token_id] * num_queries + [self.image_token_id]) * num_queries\n            tokenized_image += [self.image_token_id]\n\n        return images_list, images_crop_list, images_spatial_crop, tokenized_image\n\n    def process_single_sample(self, messages: List[Dict]) -> Dict[str, Any]:\n        images = []\n        for m in messages:\n            for img in m.get(\"images\", []) or []:\n                images.append(self.deserialize_image(img))\n\n        tokenized_str, images_seq_mask = [], []\n        images_list, images_crop_list, images_spatial_crop = [], [], []\n        prompt_token_count = -1\n        assistant_started = False\n        image_idx = 0\n\n        tokenized_str.append(self.bos_id)\n        images_seq_mask.append(False)\n\n        for m in messages:\n            role = m[\"role\"]\n            content = m[\"content\"]\n\n            if role == \"<|Assistant|>\":\n                if not assistant_started:\n                    prompt_token_count = len(tokenized_str)\n                    assistant_started = True\n                content = f\"{content.strip()} {self.tokenizer.eos_token}\"\n\n            text_splits = content.split(\"<image>\")\n            for i, text_part in enumerate(text_splits):\n                tok = text_encode(self.tokenizer, text_part, bos=False, eos=False)\n                tokenized_str.extend(tok)\n                images_seq_mask.extend([False] * len(tok))\n\n                if i < len(text_splits) - 1:\n                    img = images[image_idx]\n                    il, cl, sc, tok_img = self.process_image(img)\n                    images_list.extend(il)\n                    images_crop_list.extend(cl)\n                    images_spatial_crop.extend(sc)\n\n                    tokenized_str.extend(tok_img)\n                    images_seq_mask.extend([True] * len(tok_img))\n                    image_idx += 1\n\n        if not assistant_started:\n            prompt_token_count = len(tokenized_str)\n\n        images_ori = torch.stack(images_list, dim=0)\n        images_spatial_crop_tensor = torch.tensor(images_spatial_crop, dtype=torch.long)\n        images_crop = torch.stack(images_crop_list, dim=0) if images_crop_list else torch.zeros(\n            (1, 3, self.base_size, self.base_size), dtype=self.dtype\n        )\n\n        return {\n            \"input_ids\": torch.tensor(tokenized_str, dtype=torch.long),\n            \"images_seq_mask\": torch.tensor(images_seq_mask, dtype=torch.bool),\n            \"images_ori\": images_ori,\n            \"images_crop\": images_crop,\n            \"images_spatial_crop\": images_spatial_crop_tensor,\n            \"prompt_token_count\": prompt_token_count,\n        }\n\n    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n        batch_data = [self.process_single_sample(f[\"messages\"]) for f in features]\n\n        input_ids_list = [x[\"input_ids\"] for x in batch_data]\n        images_seq_mask_list = [x[\"images_seq_mask\"] for x in batch_data]\n        prompt_counts = [x[\"prompt_token_count\"] for x in batch_data]\n\n        input_ids = pad_sequence(input_ids_list, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n        images_seq_mask = pad_sequence(images_seq_mask_list, batch_first=True, padding_value=False)\n\n        labels = input_ids.clone()\n        labels[labels == self.tokenizer.pad_token_id] = -100\n        labels[images_seq_mask] = -100\n        for i, pc in enumerate(prompt_counts):\n            if pc > 0:\n                labels[i, :pc] = -100\n\n        attention_mask = (input_ids != self.tokenizer.pad_token_id).long()\n        images_batch = [(x[\"images_crop\"], x[\"images_ori\"]) for x in batch_data]\n        images_spatial_crop = torch.cat([x[\"images_spatial_crop\"] for x in batch_data], dim=0)\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels,\n            \"images\": images_batch,\n            \"images_seq_mask\": images_seq_mask,\n            \"images_spatial_crop\": images_spatial_crop,\n        }\n","metadata":{"id":"PLezklY0Xi6O","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T05:00:52.876084Z","iopub.execute_input":"2025-12-14T05:00:52.877046Z","iopub.status.idle":"2025-12-14T05:00:53.158820Z","shell.execute_reply.started":"2025-12-14T05:00:52.877006Z","shell.execute_reply":"2025-12-14T05:00:53.158095Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Inference and eval","metadata":{"id":"zWgEidgJXl9I"}},{"cell_type":"code","source":"# ========= (5) Inference helper =========\n@torch.inference_mode()\ndef ocr_one(model, tokenizer, image_path: str, prompt: str, base_size=1024, image_size=640, crop_mode=True) -> str:\n    out = model.infer(\n        tokenizer,\n        prompt=prompt,\n        image_file=image_path,\n        output_path=\"outputs\",\n        base_size=base_size,\n        image_size=image_size,\n        crop_mode=crop_mode,\n        save_results=False,\n        test_compress=False,\n    )\n    # robust parse\n    if isinstance(out, str):\n        return normalize_text(out)\n    if isinstance(out, list) and len(out) > 0:\n        return normalize_text(str(out[0]))\n    if isinstance(out, dict):\n        for k in [\"text\", \"result\", \"output\"]:\n            if k in out:\n                return normalize_text(str(out[k]))\n    return normalize_text(str(out))\n\ndef eval_loop(model, tokenizer, samples: List[Dict[str, str]], prompt: str,\n              base_size=1024, image_size=640, crop_mode=True, max_eval=None) -> Dict[str, float]:\n    if max_eval is not None:\n        samples = samples[:max_eval]\n\n    y_true, y_pred = [], []\n    for s in samples:\n        gt = normalize_text(s[\"text\"])\n        pr = ocr_one(model, tokenizer, s[\"image_path\"], prompt, base_size, image_size, crop_mode)\n        y_true.append(gt)\n        y_pred.append(pr)\n\n    return {\n        \"cer\": float(cer(y_true, y_pred)),\n        \"wer\": float(wer(y_true, y_pred)),\n        \"n\": len(samples),\n    }","metadata":{"id":"SSM_jT7mXvKz","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T05:00:58.805178Z","iopub.execute_input":"2025-12-14T05:00:58.805933Z","iopub.status.idle":"2025-12-14T05:00:58.813498Z","shell.execute_reply.started":"2025-12-14T05:00:58.805907Z","shell.execute_reply":"2025-12-14T05:00:58.812409Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Main","metadata":{"id":"3sOW09JWXwup"}},{"cell_type":"code","source":"# ========= (6) Main =========\ndef main1():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--data_dir\", type=str, required=True)\n    ap.add_argument(\"--out_dir\", type=str, default=\"outputs/handwriting_lora\")\n    ap.add_argument(\"--val_ratio\", type=float, default=0.05)\n    ap.add_argument(\"--seed\", type=int, default=3407)\n\n    ap.add_argument(\"--max_steps\", type=int, default=800)\n    ap.add_argument(\"--lr\", type=float, default=2e-4)\n    ap.add_argument(\"--bsz\", type=int, default=2)\n    ap.add_argument(\"--gas\", type=int, default=4)\n\n    ap.add_argument(\"--image_size\", type=int, default=640)\n    ap.add_argument(\"--base_size\", type=int, default=1024)\n    ap.add_argument(\"--crop_mode\", action=\"store_true\")\n\n    ap.add_argument(\"--max_eval\", type=int, default=200)  # eval nhanh\n    args = ap.parse_args()\n\n    set_seed(args.seed)\n    os.makedirs(args.out_dir, exist_ok=True)\n\n    # Prompt: nên “cứng” và nhất quán cho chữ viết tay\n    PROMPT = \"<image>\\nHãy chép lại nguyên văn chữ viết tay trong ảnh. Giữ đúng dấu tiếng Việt. \"\n\n    # 1) Download model snapshot\n    #snapshot_download(\"unsloth/DeepSeek-OCR\", local_dir=\"deepseek_ocr\")\n\n    # 2) Load base model\n    model, tokenizer = FastVisionModel.from_pretrained(\n        \"unsloth/DeepSeek-OCR\",\n        load_in_4bit=False,\n        auto_model=AutoModel,\n        trust_remote_code=True,\n        unsloth_force_compile=True,\n        use_gradient_checkpointing=\"unsloth\",\n    )\n\n\n    # 3) Load dataset\n    import json, random\n\n    pairs = collect_pairs(args.data_dir, recursive=True)\n    rng = random.Random(42)\n    rng.shuffle(pairs)\n\n    n_val = max(1, int(len(pairs)*0.1))\n    split = {\"train\": pairs[n_val:], \"val\": pairs[:n_val]}\n\n    # Save train/val\n    with open(\"split.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(split, f, ensure_ascii=False, indent=2)\n    train_pairs = split[\"train\"]\n    val_pairs = split[\"val\"]\n    print(f\"Total: {len(pairs)} | Train: {len(train_pairs)} | Val: {len(val_pairs)}\")\n\n    # 4) Baseline eval before finetune\n    FastVisionModel.for_inference(model)\n    base_metrics = eval_loop(\n        model, tokenizer, val_pairs, PROMPT,\n        base_size=args.base_size, image_size=args.image_size, crop_mode=args.crop_mode,\n        max_eval=args.max_eval\n    )\n    print(\"Baseline:\", base_metrics)\n\n    # 5) Add LoRA (same target_modules)\n    FastVisionModel.for_training(model)\n    model = FastVisionModel.get_peft_model(\n        model,\n        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n        r=16,\n        lora_alpha=16,\n        lora_dropout=0,\n        bias=\"none\",\n        random_state=args.seed,\n        use_rslora=False,\n        loftq_config=None,\n    )\n\n    # 6) Convert to conversations\n    train_conv = [to_conversation(s, PROMPT) for s in train_pairs]\n    val_conv = [to_conversation(s, PROMPT) for s in val_pairs]\n\n    # 7) Trainer\n    collator = DeepSeekOCRDataCollator(\n        tokenizer=tokenizer,\n        model=model,\n        image_size=args.image_size,\n        base_size=args.base_size,\n        crop_mode=args.crop_mode,\n        train_on_responses_only=True,\n    )\n\n    targs = TrainingArguments(\n        output_dir=args.out_dir,\n        per_device_train_batch_size=args.bsz,\n        gradient_accumulation_steps=args.gas,\n        learning_rate=args.lr,\n        warmup_steps=20,\n        max_steps=args.max_steps,\n        logging_steps=10,\n        save_steps=100,\n        optim=\"adamw_8bit\",\n        weight_decay=0.001,\n        lr_scheduler_type=\"linear\",\n        seed=args.seed,\n        fp16=not is_bf16_supported(),\n        bf16=is_bf16_supported(),\n        report_to=\"none\",\n        dataloader_num_workers=2,\n        remove_unused_columns=False,  # quan trọng cho vision finetune\n    )\n\n    trainer = Trainer(\n        model=model,\n        tokenizer=tokenizer,\n        data_collator=collator,\n        train_dataset=train_conv,\n        args=targs,\n    )\n\n    #trainer.train()\n    trainer.train(resume_from_checkpoint=\"/kaggle/working/checkpoint-800/checkpoint-800\")\n    print(\"Done Train\")\n    # 8) Save LoRA adapters\n    model.save_pretrained(os.path.join(args.out_dir, \"lora_model\"))\n    tokenizer.save_pretrained(os.path.join(args.out_dir, \"lora_model\"))\n\n    # 9) Eval after finetune\n    FastVisionModel.for_inference(model)\n    ft_metrics = eval_loop(\n        model, tokenizer, val_pairs, PROMPT,\n        base_size=args.base_size, image_size=args.image_size, crop_mode=args.crop_mode,\n        max_eval=args.max_eval\n    )\n    print(\"Finetuned:\", ft_metrics)\n\n    # 10) Write report\n    report = {\"baseline\": base_metrics, \"finetuned\": ft_metrics, \"train_size\": len(train_pairs), \"val_size\": len(val_pairs)}\n    with open(os.path.join(args.out_dir, \"eval_report.json\"), \"w\", encoding=\"utf-8\") as f:\n        json.dump(report, f, ensure_ascii=False, indent=2)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Se4oz-jGq239","outputId":"ac1c4199-bd9e-41a6-f02c-a97207e78c24","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T18:06:15.912839Z","iopub.execute_input":"2025-12-13T18:06:15.913034Z","iopub.status.idle":"2025-12-13T18:06:15.933235Z","shell.execute_reply.started":"2025-12-13T18:06:15.913019Z","shell.execute_reply":"2025-12-13T18:06:15.932517Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Run","metadata":{"id":"WJraBeJRYdRW"}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n  import sys\n  DATA_DIR = \"/kaggle/working/data/InkData_word_processed\"\n  sys.argv = [\"train_handwriting.py\",\n              \"--data_dir\", DATA_DIR,\n              \"--out_dir\", \"outputs/handwriting_lora\",\n              \"--max_steps\", \"1000\",\n              \"--crop_mode\"]\n  main1()\n","metadata":{"id":"Z8TFswYWWWeU","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T18:06:15.934667Z","iopub.execute_input":"2025-12-13T18:06:15.934880Z","iopub.status.idle":"2025-12-13T18:52:48.121495Z","shell.execute_reply.started":"2025-12-13T18:06:15.934865Z","shell.execute_reply":"2025-12-13T18:52:48.120486Z"}},"outputs":[{"name":"stderr","text":"You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: WARNING `trust_remote_code` is True.\nAre you certain you want to do remote code execution?\n==((====))==  Unsloth 2025.12.5: Fast Deepseekocr patching. Transformers: 4.56.2.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nUnsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n","output_type":"stream"},{"name":"stderr","text":"You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\nYou are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\nSome weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at unsloth/DeepSeek-OCR and are newly initialized: ['model.vision_model.embeddings.position_ids']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"Total: 110746 | Train: 99672 | Val: 11074\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe image you've uploaded is a simple line drawing of a person's face. The person is facing forward, with their eyes looking straight ahead. The lines are clean and smooth, giving the drawing a minimalist and modern feel. The person's hair is short and neatly styled, and their facial features are simple and uncomplicated. The image is in black and white, which adds to its clean and minimalist aesthetic. The person's expression is neutral, and they seem to be looking directly at the viewer. The overall style of the image is simple and modern, with a focus on clean lines and minimal detail.\n\n\nA simple line drawing of a banana.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA line drawing of a person with a hat on, holding a cane, and a cane in hand.\n\n\nA. B. C. D.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBaseline: {'cer': 1.1713859910581221, 'wer': 1.0, 'n': 200}\nUnsloth: Making `model.base_model.model.model` require gradients\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_774/613679430.py:115: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer._unsloth___init__`. Use `processing_class` instead.\n  trainer = Trainer(\n==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 99,672 | Num Epochs = 1 | Total steps = 1,000\nO^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n \"-____-\"     Trainable parameters = 77,509,632 of 3,413,615,872 (2.27% trained)\nWarning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: \n\tsave_steps: 100 (from args) != 200 (from trainer_state.json)\nUnsloth: Not an error, but DeepseekOCRForCausalLM does not accept `num_items_in_batch`.\nUsing gradient accumulation will be very slightly less accurate.\nRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 39:49, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>810</td>\n      <td>0.220500</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.214800</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.236600</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.230500</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.265400</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.183900</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.283900</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.209300</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.203800</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.254700</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.169600</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.181800</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.275900</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.115300</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.210200</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.188100</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.185800</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.163700</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.275800</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.160900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n","output_type":"stream"},{"name":"stdout","text":"Done Train\n","output_type":"stream"},{"name":"stderr","text":"You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n","output_type":"stream"},{"name":"stdout","text":"vẻ \nmặt \nxuống \nthuốc \nhơn \nvỏ \nbê \ngiá \nđể \nGồm \nÔng \ntâm \nTôi \nnhóm \nSức \nThật \nVùng \nphí \ncông \nchương \nhiện \ngiấy \nNgày \nbạn \nthể \nsau \n buổi \ndài \nvà \nchủ \nGiám \nlà \nmua \nđa \nquay \nNghị \nphản \nnhi \nNhân \ncao \nsẽ \ngấp \nđầm \nrồi \nchủ \nvĩnh \ntrong \nTổng \nCầu \nđi \nnhanh \nchống \nnhận \nbiết \ncác \nđể \nlời \nNơi \ncho \nmột \nchuẩn \nchiến \ncác \nThông \nbị \nLiệu \nnúi \nVăn \nnghĩa \ncuộc \ntrưởng \nquan \nđịnh \nxã \ný ý \ncơ \ncác \ntại \nMai \nở \nvắng \nTrường \ntrở \nmái \ngiúp \ngiảm \nviệc \nđứng \ntriển \nra \n3 \ncùng \ntra \ncó \nđể \nthất \nvề \nnước \ntrình \nđường \nThứ \nUBND \nnhiều \n3 \nrành \nThành \nchiến \ngiúp \nlại \nlại \nkhẩn \ntới \nvà \nYum \nhộ \nnhỏ \nĐại \nphát \nmô \nHai \ntrẻ \ndiệu \ntử \nvợ \nnguy \nkhóc \nlà \ntrưởng \nViệc \nmức \nkiếm \nnhỏ \ncấp \nsự \n1997 \nsẽ \ncâu \nxóa \nngười \nmột \nHĐND \nbỏ \nqua \ntới \ntừ \ntừ \nấy \nngẫm \nHonda \nđã \nmở \nkhoảng \nthì thì \nngười \nhai \nbà \nkhắc \nngười \ncon \nthường \ntrái \ncung \ntriệt \nNhiều \nbợ \nđi \ngiờ \nchất \ngần \nđó \n750 \nbiển \ncho \ncấp \ntân \nsố \nxác \nhết \nmảy \nNgười \ncấp \ncường \nbuồn \nlà \nhiện \nVăn \nkhấy \nkhông \nnhiệm \ntán \nvà \nthông \n2004 \ncông \nđể \n19 \nsinh \ncó \n1 \nchút \nFinetuned: {'cer': 1.1713859910581221, 'wer': 1.0, 'n': 200}\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"print(\"Done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T18:52:48.123205Z","iopub.execute_input":"2025-12-13T18:52:48.123507Z","iopub.status.idle":"2025-12-13T18:52:48.127716Z","shell.execute_reply.started":"2025-12-13T18:52:48.123488Z","shell.execute_reply":"2025-12-13T18:52:48.126961Z"}},"outputs":[{"name":"stdout","text":"Done!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!cd /kaggle/working && zip -r lora_model.zip outputs/handwriting_lora/lora_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T18:52:48.128626Z","iopub.execute_input":"2025-12-13T18:52:48.128918Z","iopub.status.idle":"2025-12-13T18:53:04.914747Z","shell.execute_reply.started":"2025-12-13T18:52:48.128891Z","shell.execute_reply":"2025-12-13T18:53:04.913966Z"}},"outputs":[{"name":"stdout","text":"  adding: outputs/handwriting_lora/lora_model/ (stored 0%)\n  adding: outputs/handwriting_lora/lora_model/tokenizer.json (deflated 79%)\n  adding: outputs/handwriting_lora/lora_model/tokenizer_config.json (deflated 97%)\n  adding: outputs/handwriting_lora/lora_model/adapter_config.json (deflated 56%)\n  adding: outputs/handwriting_lora/lora_model/special_tokens_map.json (deflated 67%)\n  adding: outputs/handwriting_lora/lora_model/README.md (deflated 65%)\n  adding: outputs/handwriting_lora/lora_model/adapter_model.safetensors (deflated 8%)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Evaluate","metadata":{}},{"cell_type":"code","source":"import json, os\n\ndef load_split(split_path):\n    with open(split_path, \"r\", encoding=\"utf-8\") as f:\n        split = json.load(f)\n    train_pairs = split[\"train\"]\n    val_pairs   = split[\"val\"]\n\n    # sanity check\n    assert isinstance(val_pairs, list) and len(val_pairs) > 0\n    assert \"image_path\" in val_pairs[0] and \"text\" in val_pairs[0]\n    print(\"Loaded split:\", split_path)\n    print(\"Train:\", len(train_pairs), \"Val:\", len(val_pairs))\n    print(\"Example:\", val_pairs[0])\n    return train_pairs, val_pairs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T05:06:32.200967Z","iopub.execute_input":"2025-12-14T05:06:32.201320Z","iopub.status.idle":"2025-12-14T05:06:32.207331Z","shell.execute_reply.started":"2025-12-14T05:06:32.201293Z","shell.execute_reply":"2025-12-14T05:06:32.206526Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from unsloth import FastVisionModel\nfrom transformers import AutoModel\nfrom peft import PeftModel\n\nBASE_ID = \"unsloth/DeepSeek-OCR\"\nLORA_DIR = \"/kaggle/working/outputs_finetune/outputs/handwriting_lora/lora_model\"\n\nmodel, tokenizer = FastVisionModel.from_pretrained(\n    BASE_ID,\n    load_in_4bit=True,\n    auto_model=AutoModel,\n    trust_remote_code=True,\n    unsloth_force_compile=True,\n)\n\nmodel = PeftModel.from_pretrained(model, LORA_DIR)\nFastVisionModel.for_inference(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T00:33:28.706501Z","iopub.execute_input":"2025-12-14T00:33:28.706783Z","iopub.status.idle":"2025-12-14T00:33:51.261703Z","shell.execute_reply.started":"2025-12-14T00:33:28.706762Z","shell.execute_reply":"2025-12-14T00:33:51.260878Z"}},"outputs":[{"name":"stderr","text":"You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: WARNING `trust_remote_code` is True.\nAre you certain you want to do remote code execution?\n==((====))==  Unsloth 2025.12.5: Fast Deepseekocr patching. Transformers: 4.56.2.\n   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"name":"stderr","text":"You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\nYou are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\nSome weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at unsloth/DeepSeek-OCR and are newly initialized: ['model.vision_model.embeddings.position_ids']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): DeepseekOCRForCausalLM(\n      (model): DeepseekOCRModel(\n        (embed_tokens): Embedding(129280, 1280)\n        (layers): ModuleList(\n          (0): DeepseekV2DecoderLayer(\n            (self_attn): LlamaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n            )\n            (mlp): DeepseekV2MLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1280, out_features=6848, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=6848, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1280, out_features=6848, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=6848, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=6848, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=6848, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): DeepseekV2RMSNorm((1280,), eps=1e-06)\n            (post_attention_layernorm): DeepseekV2RMSNorm((1280,), eps=1e-06)\n          )\n          (1-11): 11 x DeepseekV2DecoderLayer(\n            (self_attn): LlamaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n            )\n            (mlp): DeepseekV2MoE(\n              (experts): ModuleList(\n                (0-63): 64 x DeepseekV2MLP(\n                  (gate_proj): lora.Linear4bit(\n                    (base_layer): Linear4bit(in_features=1280, out_features=896, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Identity()\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1280, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=896, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (up_proj): lora.Linear4bit(\n                    (base_layer): Linear4bit(in_features=1280, out_features=896, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Identity()\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1280, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=896, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (down_proj): lora.Linear4bit(\n                    (base_layer): Linear4bit(in_features=896, out_features=1280, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Identity()\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=896, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=1280, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (act_fn): SiLU()\n                )\n              )\n              (gate): DeepseekV2MoEGate()\n              (shared_experts): DeepseekV2MLP(\n                (gate_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=1280, out_features=1792, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1792, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (up_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=1280, out_features=1792, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1792, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (down_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=1792, out_features=1280, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1792, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act_fn): SiLU()\n              )\n            )\n            (input_layernorm): DeepseekV2RMSNorm((1280,), eps=1e-06)\n            (post_attention_layernorm): DeepseekV2RMSNorm((1280,), eps=1e-06)\n          )\n        )\n        (norm): DeepseekV2RMSNorm((1280,), eps=1e-06)\n        (rotary_emb): LlamaRotaryEmbedding()\n        (sam_model): ImageEncoderViT(\n          (patch_embed): PatchEmbed(\n            (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n          )\n          (blocks): ModuleList(\n            (0-11): 12 x Block(\n              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (attn): Attention(\n                (qkv): Linear4bit(in_features=768, out_features=2304, bias=True)\n                (proj): Linear4bit(in_features=768, out_features=768, bias=True)\n              )\n              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (mlp): MLPBlock(\n                (lin1): Linear4bit(in_features=768, out_features=3072, bias=True)\n                (lin2): Linear4bit(in_features=3072, out_features=768, bias=True)\n                (act): GELU(approximate='none')\n              )\n            )\n          )\n          (neck): Sequential(\n            (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): LayerNorm2d()\n            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (3): LayerNorm2d()\n          )\n          (net_2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (net_3): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        )\n        (vision_model): VitModel(\n          (embeddings): CLIPVisionEmbeddings(\n            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n            (position_embedding): Embedding(257, 1024)\n          )\n          (transformer): NoTPTransformer(\n            (layers): ModuleList(\n              (0-23): 24 x NoTPTransformerBlock(\n                (self_attn): NoTPAttention(\n                  (qkv_proj): Linear4bit(in_features=1024, out_features=3072, bias=True)\n                  (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n                )\n                (mlp): NoTPFeedForward(\n                  (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n                  (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n                )\n                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              )\n            )\n          )\n          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n        (projector): MlpProjector(\n          (layers): Linear4bit(in_features=2048, out_features=1280, bias=True)\n        )\n      )\n      (lm_head): Linear(in_features=1280, out_features=129280, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"_, val_pairs = load_split(\"/kaggle/working/split.json\")\n\nPROMPT = \"<image>\\nHãy chép lại nguyên văn chữ viết tay trong ảnh. Giữ đúng dấu tiếng Việt. \"\n\nft_metrics = eval_loop(\n    model, tokenizer, val_pairs, PROMPT,\n    base_size=1024, image_size=640, crop_mode=True,\n    max_eval=200\n)\nprint(\"Finetuned:\", ft_metrics)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T00:34:05.219584Z","iopub.execute_input":"2025-12-14T00:34:05.220323Z","iopub.status.idle":"2025-12-14T00:41:36.755208Z","shell.execute_reply.started":"2025-12-14T00:34:05.220296Z","shell.execute_reply":"2025-12-14T00:41:36.754515Z"}},"outputs":[{"name":"stdout","text":"Loaded split: /kaggle/working/split.json\nTrain: 99672 Val: 11074\nExample: {'image_path': '/kaggle/working/data/InkData_word_processed/20150305_0036_9415_2_tg_4_1_1.png', 'text': 'vô'}\nvề \nmặt \nxưởng \nthuốc \ntản \nvỏ \nbê \ngiá \nđể \nCòn \nđộng \ntâm \nTp \nnhân \nsức \nThật \nvùng \nphí \ncông \nchương \nhơn \ngiấy \nNgày \nbạn \nthể \nsau \nbiểu \ndài \nvà \nchủ \nGiám \nlà \nmua \nđa \nquay \nVăn \nphản \nnhi \nnhân \ncao \nsẽ \ngấp \nđất \ntôi \nchủ \nrình \ntrong \nTăng \nCòn \nđi \nnhành \nchẳng \nphận \nbiết \ncác \nđể \nbởi \nHải \ncho \nmột \nchưa \nchính \ncác \nThông \nbị \nliệu \nnêu \nNăm \nnghĩa \ncuộc \ntrưởng \nquan \nđịnh \nxã \ný ý \ncơ \ncác \ntại \nMai \nở \nvắng \nTrường \ntổ \nmái \ngiúp \ngiảm \nviệc \nđứng \ntriển \nra \n3 \ncùng \ntra \ncó \nđể \nthật \nvề \nnước \ntình \ntưởng \nthứ \nUBND \nnhiều \n3 \ntành \nThành \nchiến \ngiáp \nlại \nxịt \nkhăn \ntới \nvà \ntrên \nhộ \nnhỏ \nĐại \nphát \nmô \ntại \ntư \ndiện \ntư \nvợ \nnguy \nkéo \nlà \ntrưởng \nthể \nmức \nkiếm \nnhỏ \ncấp \nsự \ntrong \nở \ncâu \nxóa \nngười \nnhật \nHĐND \nbỏ \nqua \ntử \ntừ \ntừ \ncấp \nngẫm \ntoanh \nđã \nnơi \nkhoảng \nAn \nngười \nphải \nbà \nkhắc \nngười \ncon \nthường \ntrái \nxong \ntrật \nnhiều \nbộ \nđi \ngiờ \nchất \ngần \nđó \nTôi \nbiển \ncho \ncấp \ntân \nsố \nxác \nnốt \nnhưng \ntiếp \ncấp \ncường \nbiển \nlà \nhiện \nVăn \nkhẳng \nkhông \nnhiên \ntên \nvà \nthông \n2004 \ncông \nđể \n14 \nsức \ncó \n1 \nchủ \nFinetuned: {'cer': 1.1713859910581221, 'wer': 1.0, 'n': 200}\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"del model\nimport gc, torch\ngc.collect()\ntorch.cuda.empty_cache()\n\nbase_model, base_tok = FastVisionModel.from_pretrained(\n    BASE_ID,\n    load_in_4bit=True,\n    auto_model=AutoModel,\n    trust_remote_code=True,\n    unsloth_force_compile=True,\n)\nFastVisionModel.for_inference(base_model)\n\nbase_metrics = eval_loop(\n    base_model, base_tok, val_pairs, PROMPT,\n    base_size=1024, image_size=640, crop_mode=True,\n    max_eval=200\n)\nprint(\"Baseline:\", base_metrics)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T00:42:18.390025Z","iopub.execute_input":"2025-12-14T00:42:18.390340Z","iopub.status.idle":"2025-12-14T01:08:34.906257Z","shell.execute_reply.started":"2025-12-14T00:42:18.390315Z","shell.execute_reply":"2025-12-14T01:08:34.905637Z"}},"outputs":[{"name":"stderr","text":"You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: WARNING `trust_remote_code` is True.\nAre you certain you want to do remote code execution?\n==((====))==  Unsloth 2025.12.5: Fast Deepseekocr patching. Transformers: 4.56.2.\n   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"name":"stderr","text":"You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\nYou are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\nSome weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at unsloth/DeepSeek-OCR and are newly initialized: ['model.vision_model.embeddings.position_ids']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"A curved line that resembles a smiley face.\nA. mă\nXimong\n\nA line drawing of a diamond shape with a line through the center.\nA curved line that is part of the letter \"C\".A curved line that is part of the letter \"C\".A curved line that is part of a circle.\n\nA\nA. B. C. D.\nA. CóB. Không\nA: Cô, có bạn gặp lại không?B: Có, tôi đã gặp bạn lâu rồi.\n\n\nA line drawing of a handwritten signature in the style of a person's signature.\n\n\n\n\nA simple line drawing of a cat's face.\n\nA curved line that resembles a smiley face.\nA. Các chữ viết tay trong ảnh là: C, Đ, Đ, Đ, Đ, Đ, Đ, Đ, Đ, Đ, Đ, C, Đ, Đ, Đ, Đ, Đ, Đ, Đ, Đ, Đ.\nA\n\n\n\n\n\n\nA. Đầu tiên, chép lại nguyên văn chữ viết tay trong ảnh. B. Đầu tiên, chép lại nguyên văn chữ viết tay. C. Đầu tiên, chép lại nguyên văn chữ viết tay, đúng dấu tiếng Việt. D. Đầu tiên, chép lại nguyên văn chữ viết tay đúng dấu tiếng Việt.\nA: Có - là - mẹA: Có - là - mẹA: Có - là - mẵA: Có - là - mẹA: Có - là - mấA: Có - là - mẹA: Có - là - mặA: Có - là - mẹA: Có - là - mẸA: Có - là - mẹA: Có - là - mẺA: Có - là - mẹA: Có - là - mẼA: Có - là - mẹA: Có - là - mẽA: Có - là - mẹA: Có - là - mẾA: Có - là - mẹA: Có - là - mếA: Có - là - mẹA: Có - là - mẻA: Có - là - mẹA: Có - là - mẴA: Có - là - mẹA: Có - là - mẶA: Có - là - mẹA: Có - là - mẲA: Có - là - mẹA: Có - là - mẳA: Có - là - mẹA: Có - là - mằA: Có - là - mẹA: Có - là - mắA: Có - là - mẹA: Có - là - mẰA: Có - là - mẹA: Có - là - mẮA: Có - là - mẹA: Có - là - mảA: Có - là - mẹA: Có - là - mậA: Có - là - mẹA: Có - là - mẪA: Có - là - mẹA: Có - là - mẫA: Có - là - mẹA: Có - là - mẬA: Có - là - mẹA: Có - là - mẠA: Có - là - mẹA: Có - là - mạA: Có - là - mẹA: Có - là - mẢA: Có - là - mẹA: Có - là - mẘA: Có - là - mẹA: Có - là - mẙA: Có - là - mẹA: Có - là - mẚA: Có - là - mẹA: Có - là - mẛA: Có - là - mẹA: Có - là - mẜA: Có - là - mẹA: Có - là - mẝA: Có - là - mẹA: Có - là - mẞA: Có - là - mẹA: Có - là - mẟA: Có - là - mẹA: Có - là - mẗA: Có - là - mẹA: Có - là - mẖA: Có - là - mẹA: Có - là - mẕA: Có - là - mẹA: Có - là - mẑA: Có - là - mẹA: Có - là - mẒA: Có - là - mẹA: Có - là - mẏA: Có - là - mẹA: Có - là - mẓA: Có - là - mẹA: Có - là - mẎA: Có - là - mẹA: Có - là - mẍA: Có - là - mẹA: Có - là - mẊA: Có - là - mẹA: Có - là - mẉA: Có - là - mẹA: Có - là - mẋA: Có - là - mẹA: Có - là - mẌA: Có - là - mẹA: Có - là - mẁA: Có - là - mẹA: Có - là - mẃA: Có - là - mẹA: Có - là - mẄA: Có - là - mẹA: Có - là - mẅA: Có - là - mẹA: Có - là - mẆA: Có - là - mẹA: Có - là - mẇA: Có - là - mẹA: Có - là - mẈA: Có - là - mẹA: Có - là - mẂA: Có - là - mẹA: Có - là - mẀA: Có - là - mẹA: Có - là - mẉ\n\nCác từ trong ảnh đều có ý nghĩa đúng và đúng trong ảnh.\nA line drawing of a pair of scissors.\n\nA\n\nA curved line resembling a capital letter \"C\" or a backwards letter \"G\".A curved line resembling a capital letter \"C\" or a backwards letter \"G\".A curved shape resembling a capital letter \"C\" or a backwards letter \"G\".\n\nA line drawing of a wave.\n\nA curved line resembling a smiley face.\nA curved line resembling a smiley face.\nA: B:\nA. Các bước của chữ viết tayA. Các bước của chữ viết tayB. Các bước của chữ viết tayB. Các bước của đúng dấu tiếng ViệtC. Các bước của đúng dấu tiếng ViệtD. Các bước của đúng dấu tiếng Việt\nA: Có.\n\n\nA\nA curved line resembling a wave.\nA. Điều kiện chữ viết tay trong ảnhB. Điều kiện chữ viết tay trong ảnhC. Điều kiện chữ viết tay trong ảnhD. Điều kiện chữ viết tay trong ảnh\nA curved line that is part of the letter \"C\".\n\nA. B. C. D.\nA line drawing of a butterfly.\nA. Đọc viết tayB. Đọc viết tayC. Đọc viết tayD. Đọc viết tay\nA. Để có thể đọc được những từ trong ảnh, chúng ta phải đọc qua đâu?B. Để có thể đọc qua đâu, chúng ta phải đọc qua đâu?C. Để có thể đọc qua đâu, chúng ta phải đọc qua các từ trong ảnh?D. Để có thể đọc qua đâu, chúng ta phải đọc qua từ trong ảnh?\nA. B. C. D.\nA curved line that resembles a smiley face.\n\nA. B. C. D.\nA: Có. B: Có. C: Có. D: Có.\n\n\n\nA: Tôi có thể hỗ trợ bạn không?B: Tôi không thể.\n\nA: Điều này không có ý nghĩa gì. B: Điều này không có ý nghĩa gì. A: Điều này không có ý nghĩa gì. B: Điền từ thích hợp vào chỗ trống: 1. Điều này không có ý nghĩa gì. 2. Điều này không có ý nghĩa gì. 3. Điều này không có ý nghĩa gì. 4. Điều này không có ý nghĩa gì.\n\nA line drawing of a handwritten signature in the style of a person.\nA handwritten note with the word \"mua\" written in cursive.\nCó đúng khôngCóCóCóCóCóCóCóCóCóCóC\n\n\nA line drawing of a handwritten signature in the style of a person.\n\n\nA line drawing of a dogA line drawing of a dogA line drawing of a dogA line drawing of a dogA\n\nA curved line that starts from the top left corner and ends at the bottom right corner.\n\n\nA. Để đọc được những từ trong ảnh, bạn cần phải chép lại nguyên văn chữ viết tay trong ảnh.B. Để đọc được những từ trong ảnh, bạn cần phải chấp nhận nguyên văn chữ viết tay trong ảnh.\n\nA: Tôi có thể đi bộ với bạn không?B: Tôi có thể đi bộ với bạn không.\n\n\nA: B:\nA: Có. B: Có.\nA. Đâu là đúng đắn?B. Đâu là đúng đắn?C. Đâu là đúng đắn?D. Đâu là đúng đắn?A. Đâu là đúng đắn?B. Đâu là đúng đăn?C. Đâu là đúng đăn?D. Đâu là đúng đăn?A. Đâu là đúng đăn?B. Đâu là đúng đăn?C. Đâu là đóng dấu?D. Đâu là đóng dấu?A. Đâu là đúng đăn?B. Đâu là đúng dấu?C. Đâu là đúng dấu?D. Đâu là đúng dấu?A. Đâu là đúng đăn?B. Đâu không là đúng đăn?C. Đâu là đúng đăn?D\n\nA. B. C. D.\nA: Có. B: Không.\n\nA. Tôi có thể đưa ra một số thông tin về tôi và tôi có thể đưa ra một số thông tin về bạn. B. Tôi có thể đưa ra một số thông tin về tôi và tài khoản của tôi. C. Tôi có thể đưa ra một số thông tin về tôi và tình yêu của tôi. D. Tôi có thể đưa ra một số thông tin về tôi và tội của tôi.\nCó 2 ảnh có chữ viết tay trong ảnh.\nA\n\n\n\nA: B:\n\n\n\n\n\nA. Tôi có. B. Tôi có.\n\n\nA line drawing of a handwritten note with a capital letter \"S\" at the beginning and a capital letter \"D\" at the end.\nA line drawing of a handwritten signature in the style of a person.\nA hand with a peace sign.\n\nA. B. C. D.\n\n\nA curved line that is part of the letter \"S\".\n\nA curved line that resembles a smiley face.\n\nA curved line resembling a wave.\nA. B. C. D.\nA curved line that resembles a smiley face.\nA: B:\nA curved line that is part of the letter \"C\" in the word \"Cup\".A curved line that is part of the letter \"C\" in the word \"Cup\".\n\n\n\n\n\n\n\n\nA curved line resembling a lowercase letter \"v\" or a backwards letter \"n\".\nA: Có. B: Có. C: Có. D: Có.\n\nA. 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, ","output_type":"stream"},{"name":"stderr","text":"This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (8192). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n","output_type":"stream"},{"name":"stdout","text":"2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, A curved line resembling a crescent or a half-moon.\nA curved line is written in the image. The correct punctuation mark is required.\n\nA line drawing of a flower.\nA line drawing of a handwritten signature in the style of a person.\nA. B. C. D.\n\nA line drawing of a handwritten signature in the style of a 19th-century French document.\n\n\n\nA line drawing of a capital letter \"M\" with a circle above it and a line below it.\nA: B:\nA. Động viết tay trong ảnh là: B. Động viết tay trong ảnh là:\nA\n\nA handwritten note with the word \"diamond\" written in cursive.\n\n\nA line drawing of a person's face.\n\n\n\nA\n\n\n\nA: Đây là một bộ phận của một người. B: Đây là một bộ phận của một người. A: Đây là một bộ phận của một người. B: Đây là bộ phận của một người. A: Đây là một bộ phận đúng tiếng Việt. B: Đây là một bộ phận đúng tiếng Việt. A: Đây là một bộ phận đúng tiếng Việt.\n\n\n\nA. C. C. C. C. C. C. C. C. C. C. B. C. C. C. C. C. C. C. C. C. A. C. C. C. C. C. C. C. C. C. D. C. C. C. C. C. C. C. C. C. E. C. C. C. C. C. C. C. C. C. F. C. C. C. C. C. C. C. C. C. G. C. C. C. C. C. C. C. C. C. H. C. C. C. C. C. C. C. C. C. I. C. C. C. C. C. C. C. C. C. J. C. C. C. C. C. C. C. C. C. K. C. C. C. C. C. C. C. C. C. L. C. C. C. C. C. C. C. C. C. M. C. C. C. C. C. C. C. C. C. N. C. C. C. C. C. C. C. C. C. O. C. C. C. C. C. C. C. C. C. P. C. C. C. C. C. C. C. C. C. Q. C. C. C. C. C. C. C. C. C. R. C. C. C. C. C. C. C. C. C. S. C. C. C. C. C. C. C. C. C. T. C. C. C. C. C. C. C. C. C. U. C. C. C. C. C. C. C. C. C. V. C. C. C. C. C. C. C. C. C. W. C. C. C. C. C. C. C. C. C. X. C. C. C. C. C. C. C. C. C. Y. C. C. C. C. C. C. C. C. C. Z. C. C. C. C. C. C. C. C. C. {C. C. C. C. C. C. C. C. C. C. [C. C. C. C. C. C. C. C. C. C, C. C. C. C. C. C. C. C. C. C, {C. C. C. C. C. C. C. C. C. {C, C. C. C. C. C. C. C. C. {C.\nA cursive script of the Vietnamese word for \"love\".\nA. Đầu tiên, chép lại nguyên văn chữ viết tay trong ảnh. B. Đầu tiên, chép lại nguyên văn chữ viết tay. C. Đầu tiên, chép lại nguyên văn chữ viết tay và đúng dấu tiếng Việt. D. Đầu tiên, chép lại nguyên văn chữ viết tay, đúng dấu tiếng Việt.\n\nA. C. C. C. C. C. C. C. C. C. C.\nA line drawing of a flower with a curved stem and two leaves.\nA curved line that resembles a smiley face.\n\nA. CóB. CóC. CóD. Có\nA. CóB. KhôngC. Có và khôngD. Không và không\n\n\n\nA line drawing of a pair of scissors.\nA\nA: Cô ơi, tôi đã mua một bộ phim. B: Điều đó là một điều không phải là người. A: Tôi không phải là người. B: Tôi không phải là người. A: Tôi không phải là người. B. Tôi không phải là người. A: Tôi không phải là người.B: Tôi không phải là người. A: Tôi không phải là ng\nA. Đọc văn chữ viết tayB. Đọc văn chữ viết tayC. Đọc văn chữ viết tayD. Đọc văn chữ viết tay\nA\n\nA. Văn chữ viết tayB. Văn chữ viết tayC. Văn chữ viết tayD. Văn chữ viết tay\nA. Đọc và chép lại nguyên văn chữ viết tay trong ảnh. B. Đọc và chép lại nguyên văn chữ viết tay trong Ảnh. C. Đọc và chép lại nguyên văn chữ viết tay trong ấn. D. Đọc và chép lại nguyên văn chữ viết tay trong ẩn.\n\nA: B:\n\nA curved line that is part of the letter \"C\".\n\n\nA. CóB. CóC. CóD. Có\nA\n\nA line drawing of a pair of scissors.\nA line drawing of a circle.\n\nA line drawing of a handwritten signature in the style of a person.\nBaseline: {'cer': 1.1713859910581221, 'wer': 1.0, 'n': 200}\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## Eval 2","metadata":{}},{"cell_type":"code","source":"!pip -q install rapidfuzz","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T01:22:26.557131Z","iopub.execute_input":"2025-12-14T01:22:26.557436Z","iopub.status.idle":"2025-12-14T01:22:29.736581Z","shell.execute_reply.started":"2025-12-14T01:22:26.557415Z","shell.execute_reply":"2025-12-14T01:22:29.735750Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from rapidfuzz.distance import Levenshtein\n\ndef char_stats(gt: str, pr: str):\n    # Levenshtein distance = S + D + I\n    dist = Levenshtein.distance(gt, pr)\n    n = max(len(gt), 1)\n    cer = dist / n\n    sim = max(0.0, 1.0 - cer)  # similarity\n    return dist, cer, sim\n\ndef eval_loop_plus(model, tokenizer, samples, prompt,\n                   base_size=1024, image_size=640, crop_mode=True, max_eval=None):\n    import random\n    if max_eval is not None and len(samples) > max_eval:\n        samples = random.sample(samples, max_eval)\n\n    y_true, y_pred = [], []\n    exact = 0\n    total_chars = 0\n    total_dist = 0\n    total_sim = 0.0\n\n    for s in samples:\n        gt = normalize_text(s[\"text\"])\n        pr = ocr_one(model, tokenizer, s[\"image_path\"], prompt, base_size, image_size, crop_mode)\n\n        y_true.append(gt); y_pred.append(pr)\n\n        if pr == gt:\n            exact += 1\n\n        dist, cer_i, sim_i = char_stats(gt, pr)\n        total_chars += max(len(gt), 1)\n        total_dist += dist\n        total_sim += sim_i\n\n    # CER/WER gốc của bạn\n    CER = float(cer(y_true, y_pred))\n    WER = float(wer(y_true, y_pred))\n\n    return {\n        \"cer\": CER,\n        \"wer\": WER,\n        \"exact_match\": exact / len(samples),\n        \"avg_similarity\": total_sim / len(samples),              # (0..1)\n        \"char_accuracy_like\": 1.0 - (total_dist / total_chars),  # (0..1) gần nghĩa “ký tự đúng”\n        \"exact_word\": exact,\n        \"n\": len(samples),\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T01:22:29.737664Z","iopub.execute_input":"2025-12-14T01:22:29.738073Z","iopub.status.idle":"2025-12-14T01:22:29.746496Z","shell.execute_reply.started":"2025-12-14T01:22:29.738027Z","shell.execute_reply":"2025-12-14T01:22:29.745918Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from unsloth import FastVisionModel\nfrom transformers import AutoModel\nfrom peft import PeftModel\n\nBASE_ID = \"unsloth/DeepSeek-OCR\"\nLORA_DIR = \"/kaggle/working/outputs_finetune/outputs/handwriting_lora/lora_model\"\n\nmodel, tokenizer = FastVisionModel.from_pretrained(\n    BASE_ID,\n    load_in_4bit=True,\n    auto_model=AutoModel,\n    trust_remote_code=True,\n    unsloth_force_compile=True,\n    device_map={\"\": 0},   # ép tất cả lên GPU0, không offload CPU/disk\n)\n\nmodel = PeftModel.from_pretrained(model, LORA_DIR)\nFastVisionModel.for_inference(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T01:22:29.747292Z","iopub.execute_input":"2025-12-14T01:22:29.747700Z","iopub.status.idle":"2025-12-14T01:23:20.866783Z","shell.execute_reply.started":"2025-12-14T01:22:29.747651Z","shell.execute_reply":"2025-12-14T01:23:20.866005Z"}},"outputs":[{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/unsloth/DeepSeek-OCR:\n- conversation.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/unsloth/DeepSeek-OCR:\n- deepencoder.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/unsloth/DeepSeek-OCR:\n- modeling_deepseekocr.py\n- conversation.py\n- deepencoder.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nYou are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: WARNING `trust_remote_code` is True.\nAre you certain you want to do remote code execution?\n==((====))==  Unsloth 2025.12.5: Fast Deepseekocr patching. Transformers: 4.56.2.\n   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"name":"stderr","text":"You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\nYou are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71b72d3b22d14ab5899c3bb9c98cfdb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-000001.safetensors:   0%|          | 0.00/6.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12f56575f81749a68ccd76382d7f0b04"}},"metadata":{}},{"name":"stderr","text":"Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at unsloth/DeepSeek-OCR and are newly initialized: ['model.vision_model.embeddings.position_ids']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9306fab1d6a4a869524f55b084003a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"965a9e0d6ed341e79be279b43b42c46d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/801 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"250998b8f69c47748ea2815747c7d442"}},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): DeepseekOCRForCausalLM(\n      (model): DeepseekOCRModel(\n        (embed_tokens): Embedding(129280, 1280)\n        (layers): ModuleList(\n          (0): DeepseekV2DecoderLayer(\n            (self_attn): LlamaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n            )\n            (mlp): DeepseekV2MLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1280, out_features=6848, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=6848, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1280, out_features=6848, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=6848, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=6848, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=6848, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): DeepseekV2RMSNorm((1280,), eps=1e-06)\n            (post_attention_layernorm): DeepseekV2RMSNorm((1280,), eps=1e-06)\n          )\n          (1-11): 11 x DeepseekV2DecoderLayer(\n            (self_attn): LlamaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n            )\n            (mlp): DeepseekV2MoE(\n              (experts): ModuleList(\n                (0-63): 64 x DeepseekV2MLP(\n                  (gate_proj): lora.Linear4bit(\n                    (base_layer): Linear4bit(in_features=1280, out_features=896, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Identity()\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1280, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=896, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (up_proj): lora.Linear4bit(\n                    (base_layer): Linear4bit(in_features=1280, out_features=896, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Identity()\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1280, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=896, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (down_proj): lora.Linear4bit(\n                    (base_layer): Linear4bit(in_features=896, out_features=1280, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Identity()\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=896, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=1280, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (act_fn): SiLU()\n                )\n              )\n              (gate): DeepseekV2MoEGate()\n              (shared_experts): DeepseekV2MLP(\n                (gate_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=1280, out_features=1792, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1792, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (up_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=1280, out_features=1792, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1792, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (down_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=1792, out_features=1280, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1792, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act_fn): SiLU()\n              )\n            )\n            (input_layernorm): DeepseekV2RMSNorm((1280,), eps=1e-06)\n            (post_attention_layernorm): DeepseekV2RMSNorm((1280,), eps=1e-06)\n          )\n        )\n        (norm): DeepseekV2RMSNorm((1280,), eps=1e-06)\n        (rotary_emb): LlamaRotaryEmbedding()\n        (sam_model): ImageEncoderViT(\n          (patch_embed): PatchEmbed(\n            (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n          )\n          (blocks): ModuleList(\n            (0-11): 12 x Block(\n              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (attn): Attention(\n                (qkv): Linear4bit(in_features=768, out_features=2304, bias=True)\n                (proj): Linear4bit(in_features=768, out_features=768, bias=True)\n              )\n              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (mlp): MLPBlock(\n                (lin1): Linear4bit(in_features=768, out_features=3072, bias=True)\n                (lin2): Linear4bit(in_features=3072, out_features=768, bias=True)\n                (act): GELU(approximate='none')\n              )\n            )\n          )\n          (neck): Sequential(\n            (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): LayerNorm2d()\n            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (3): LayerNorm2d()\n          )\n          (net_2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (net_3): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        )\n        (vision_model): VitModel(\n          (embeddings): CLIPVisionEmbeddings(\n            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n            (position_embedding): Embedding(257, 1024)\n          )\n          (transformer): NoTPTransformer(\n            (layers): ModuleList(\n              (0-23): 24 x NoTPTransformerBlock(\n                (self_attn): NoTPAttention(\n                  (qkv_proj): Linear4bit(in_features=1024, out_features=3072, bias=True)\n                  (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n                )\n                (mlp): NoTPFeedForward(\n                  (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n                  (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n                )\n                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              )\n            )\n          )\n          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n        (projector): MlpProjector(\n          (layers): Linear4bit(in_features=2048, out_features=1280, bias=True)\n        )\n      )\n      (lm_head): Linear(in_features=1280, out_features=129280, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"_, val_pairs = load_split(\"/kaggle/working/split.json\")\n\nPROMPT = \"<image>\\nHãy chép lại nguyên văn chữ viết tay trong ảnh. Giữ đúng dấu tiếng Việt. \"\n\nmetrics_plus = eval_loop_plus(model, tokenizer, val_pairs, PROMPT, max_eval=200,\n                         base_size=1024, image_size=640, crop_mode=True)\nprint(f\"Metric have cor word {metrics_plus}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T01:27:37.437399Z","iopub.execute_input":"2025-12-14T01:27:37.437695Z","iopub.status.idle":"2025-12-14T01:35:12.852489Z","shell.execute_reply.started":"2025-12-14T01:27:37.437658Z","shell.execute_reply":"2025-12-14T01:35:12.851864Z"}},"outputs":[{"name":"stdout","text":"Loaded split: /kaggle/working/split.json\nTrain: 99672 Val: 11074\nExample: {'image_path': '/kaggle/working/data/InkData_word_processed/20150305_0036_9415_2_tg_4_1_1.png', 'text': 'vô'}\n","output_type":"stream"},{"name":"stderr","text":"The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"xe \nhợi \nhạ \nKhông \nquen \ncũng \nmới \nchức \nvới \nlà \nlượn \ncủa \nchận \nXanh \nnội \ntay \nđồng \nBản \nxác \nvã \nVN \nđã \nHải \nkhi \n1 \nnơi \nthời \ncó \nkhách \nở \nđặc \nsố \ndám \ncùng \ncấp \nRõ \nquan \nbỏ \nchở \nUBND \nbiết \nhai \ncảnh \nđến \ncó \nbàn \nmà \nlà \nHCM \ntâm \nvừa \ntìm \nxin \nnước \ntục \ntrách \nkiên \nnhững \nhàng \năn \npháp \nở \nmẹ \nMaestic \nnước \nChiếc \ntn \nliên \ncủa \n2 \nnước \nhân \nvụ \nbiên \nhình \nra \nba \ntrong \nxác \nhân \nkhi \nlâu \nnhận \ntên \nmê \nphòng \nTB \ncòn \ngia \nđất \nBa \ngiao \nvăn \nnày \nhiện \ncơ \nhóa \nhoảng \ncó \nquai \nnghiệp \ntâu \nđi \ntiến \nmột \nSơn \nnhà \nsự \nthể \ngian \nquý \nTriết \ndý \nthay \nđai \nxã \ntra \nđặt \ntâm \ntư \nanh \nphạm \nbóng \nnơi \nhỏa \nXuân \nBình \ntrái \nbộ \nlu \nquốc \nlên \nquá \nchủng \nngười \nkhi \nthật \nHải \ncấp \nđịa \ncho \nngò \nnay \nđạm \nđơn \ncác \ntrong \nnhững \nkhác \nvậy \nBộ \nđược \nlại \nlà \nđồng \nđầu \ntổng \nbếp \nXị \ntrưc \ngặp \nBa \nVăn \nbiển \nbao \ncãi \ngia \nthịt \ngiao \nHĐND \ncủa \ncông \nĐã \nSai \ncậu \nlý \ncùng \nnhững \n000 \nquan \nvị \nđã \nđá \nThắng \ntệ \ntriển \nsản \nbiển \ntrực \nkhu \ntục \nchất \nphát \ntin \ntrong \nbảy \nnên \nNhư \nVi \ntôi \nMetric have cor word {'cer': 1.2058823529411764, 'wer': 1.0, 'exact_match': 0.0, 'avg_similarity': 0.035375000000000004, 'char_accuracy_like': -0.2058823529411764, 'exact_word': 0, 'n': 200}\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"report = {\n    \"finetuned\": ft_metrics,\n    \"finetuned_metric\": metrics_plus,\n    \"Baseline\": base_metrics,\n    \"val_size\": len(val_pairs),\n    \"split_path\": SPLIT_PATH,\n    \"lora_dir\": LORA_DIR,\n}\nout_path = \"/kaggle/working/outputs_finetune/handwriting_lora/eval_report_reload.json\"\nos.makedirs(os.path.dirname(out_path), exist_ok=True)\n\nwith open(out_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(report, f, ensure_ascii=False, indent=2)\n\nprint(\"Wrote:\", out_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T01:23:20.925562Z","iopub.status.idle":"2025-12-14T01:23:20.925912Z","shell.execute_reply.started":"2025-12-14T01:23:20.925742Z","shell.execute_reply":"2025-12-14T01:23:20.925758Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference Test","metadata":{}},{"cell_type":"code","source":"import os, json, argparse, unicodedata\nimport torch\nfrom unsloth import FastVisionModel\nfrom transformers import AutoModel\nfrom peft import PeftModel\n\ndef normalize_text1(s: str) -> str:\n    s = unicodedata.normalize(\"NFC\", s or \"\")\n    s = \" \".join(s.split())\n    return s.strip()\n\ndef collect_image_paths1(path: str, recursive: bool = False):\n    exts = (\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\")\n    if os.path.isfile(path):\n        if path.lower().endswith(exts):\n            return [path]\n        raise ValueError(f\"--image_dir is a file but not an image: {path}\")\n\n    if os.path.isdir(path):\n        img_paths = []\n        if recursive:\n            for root, _, files in os.walk(path):\n                for f in sorted(files):\n                    if f.lower().endswith(exts):\n                        img_paths.append(os.path.join(root, f))\n        else:\n            for f in sorted(os.listdir(path)):\n                if f.lower().endswith(exts):\n                    img_paths.append(os.path.join(path, f))\n        return img_paths\n\n    raise FileNotFoundError(f\"--image_dir not found: {path}\")","metadata":{"id":"Qo1uhDiQAqdF","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T18:53:04.915840Z","iopub.execute_input":"2025-12-13T18:53:04.916092Z","iopub.status.idle":"2025-12-13T18:53:04.924397Z","shell.execute_reply.started":"2025-12-13T18:53:04.916069Z","shell.execute_reply":"2025-12-13T18:53:04.923537Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"@torch.inference_mode()\ndef main2():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--image_dir\", required=True, help=\"Path to an image file OR a directory of images\")\n    ap.add_argument(\"--lora_dir\", required=True, help=\"Path to LoRA adapter folder (e.g. outputs/handwriting_lora/lora_model)\")\n    ap.add_argument(\"--out_json\", default=\"preds.json\")\n    ap.add_argument(\"--base_model\", default=\"unsloth/DeepSeek-OCR\")\n    ap.add_argument(\"--base_size\", type=int, default=1024)\n    ap.add_argument(\"--image_size\", type=int, default=640)\n    ap.add_argument(\"--crop_mode\", action=\"store_true\")\n    ap.add_argument(\"--recursive\", action=\"store_true\", help=\"If image_dir is a folder, scan recursively\")\n    ap.add_argument(\"--output_dir\", default=\"outputs_infer\", help=\"Temp folder for model.infer output_path\")\n    args = ap.parse_args()\n\n    PROMPT = \"<image>\\nHãy chép lại nguyên văn chữ viết tay trong ảnh. Giữ đúng dấu tiếng Việt. \"\n\n    # 1) Resolve images\n    img_paths = collect_image_paths1(args.image_dir, recursive=args.recursive)\n    if len(img_paths) == 0:\n        raise RuntimeError(f\"No images found in {args.image_dir}\")\n\n    os.makedirs(args.output_dir, exist_ok=True)\n\n    # 2) Load base model\n    model, tokenizer = FastVisionModel.from_pretrained(\n        args.base_model,\n        load_in_4bit=False,\n        auto_model=AutoModel,\n        trust_remote_code=True,\n        unsloth_force_compile=True,\n    )\n\n    # 3) Apply LoRA adapter\n    # args.lora_dir should be the folder you saved with:\n    # model.save_pretrained(outputs/handwriting_lora/lora_model)\n    model = PeftModel.from_pretrained(model, args.lora_dir)\n\n    # 4) Inference mode\n    FastVisionModel.for_inference(model)\n\n    # 5) Run inference\n    results = {}\n    for p in img_paths:\n        fname = os.path.basename(p)\n        out = model.infer(\n            tokenizer,\n            prompt=PROMPT,\n            image_file=p,\n            output_path=args.output_dir,\n            base_size=args.base_size,\n            image_size=args.image_size,\n            crop_mode=args.crop_mode,\n            save_results=False,\n            test_compress=False,\n        )\n        results[fname] = normalize_text1(out if isinstance(out, str) else str(out))\n\n    # 6) Save JSON\n    with open(args.out_json, \"w\", encoding=\"utf-8\") as fp:\n        json.dump(results, fp, ensure_ascii=False, indent=2)\n    show_image_with_text(fname, results[fname])\n    print(f\"Done. Wrote {len(results)} predictions to: {args.out_json}\")\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\ndef show_image_with_text(img_path, text, figsize=(6,6)):\n    img = Image.open(f\"/kaggle/working/data/InkData_word_processed/{img_path}\").convert(\"RGB\")\n    plt.figure(figsize=figsize)\n    plt.imshow(img)\n    plt.axis(\"off\")\n    plt.title(text, fontsize=12, wrap=True)\n    plt.show()\n\nif __name__ == \"__main__\":\n    import sys\n    sys.argv = [\"infer_handwriting.py\",\n                \"--image_dir\", \"/kaggle/working/data/InkData_word_processed/20140603_0003_BCCTC_tg_0_0_0.png\",\n                \"--lora_dir\", \"outputs/handwriting_lora/lora_model\",\n                \"--out_json\", \"pred_one.json\",\n                \"--base_size\", \"1024\",\n                \"--image_size\", \"640\",\n                \"--crop_mode\"]\n    main2()\n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T19:00:15.920164Z","iopub.execute_input":"2025-12-13T19:00:15.920510Z","iopub.status.idle":"2025-12-13T19:00:39.603960Z","shell.execute_reply.started":"2025-12-13T19:00:15.920487Z","shell.execute_reply":"2025-12-13T19:00:39.602858Z"}},"outputs":[{"name":"stderr","text":"You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: WARNING `trust_remote_code` is True.\nAre you certain you want to do remote code execution?\n==((====))==  Unsloth 2025.12.5: Fast Deepseekocr patching. Transformers: 4.56.2.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nUnsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n","output_type":"stream"},{"name":"stderr","text":"You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\nYou are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\nSome weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at unsloth/DeepSeek-OCR and are newly initialized: ['model.vision_model.embeddings.position_ids']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Bản \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 600x600 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAeQAAAD4CAYAAAA9zZWtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA3BklEQVR4nO3deXRU9f0//uedyWQm+x4SCCEkMSRAAhgSNg1LQKuAoAVES4vVKtWj4qlotfppv59a2+rnaO3yaT9FbQWtCoi4hEVAJBhZI0aWQDbIRvbJRjJLZua+f3/o5AeKCMlM7p2Z5+McDjCZ3HnN+pz3+74XSQghQERERIrSKF0AERERMZCJiIhUgYFMRESkAgxkIiIiFWAgExERqQADmYiISAUYyERERCrAQCYiIlIBBjIREZEKMJCJiIhUgIFMpIDXXnsNkiTBYDDg3Llz3/r5rFmzMH78eAUqIyKlMJCJFGS1WvHHP/5R6TKISAUYyEQKmjhxIl5++WU0NDQoXQoRKYyBTKSgX/3qV3A4HN/bSrbb7XjmmWeQkpICvV6PpKQk/OpXv4LVar3oeklJSViwYAGKioqQm5sLg8GA5ORkrF+//lvH7OzsxCOPPIKRI0dCr9cjNTUVzz33HGRZdul9JKIrw0AmUtDo0aPxk5/85HtbyT/72c/w61//Gtdeey3+9Kc/YebMmfjDH/6A5cuXf+u6lZWVWLJkCebNm4cXXngBERERuOuuu3Dy5Mn+65hMJsycORNvvPEGfvKTn+Avf/kLZsyYgSeffBK/+MUv3HJfieh7CCIacv/+978FAHHkyBFRVVUl/Pz8xMMPP9z/85kzZ4px48YJIYQoKSkRAMTPfvazi46xZs0aAUDs2bOn/7JRo0YJAGLfvn39l7W0tAi9Xi8effTR/sueeeYZERQUJMrLyy865hNPPCG0Wq2ora116f0lou/HFjKRwpKTk/HjH/8Ya9euRWNj47d+vm3bNgD4Vsv10UcfBQBs3br1osvHjh2L66+/vv//MTExGDNmDM6cOdN/2aZNm3D99dcjIiICbW1t/X/mzp0Lh8OBffv2uez+EdGVYSATqcDTTz8Nu91+yXPJNTU10Gg0SE1NvejyuLg4hIeHo6am5qLLExMTv3WMiIgIdHR09P+/oqICO3bsQExMzEV/5s6dCwBoaWlxxd0ioqvgp3QBRPRVK3nFihVYu3YtnnjiiUteR5KkKzqWVqu95OVCiP5/y7KMefPm4fHHH7/kddPS0q7otojIdRjIRCrx9NNP44033sBzzz130eWjRo2CLMuoqKhARkZG/+XNzc3o7OzEqFGjrvq2UlJS0NPT098iJiLlscuaSCVSUlKwYsUK/POf/0RTU1P/5TfffDMA4KWXXrro+i+++CIAYP78+Vd9W8uWLcOBAwfw0UcffetnnZ2dsNvtV31MIhoctpCJVOSpp57C66+/jrKyMowbNw4AMGHCBKxcuRJr165FZ2cnZs6cicOHD2PdunVYvHgxZs+efdW389hjj+GDDz7AggULcNdddyE7Oxu9vb04fvw43nnnHVRXVyM6OtrVd4+ILoOBTKQiqampWLFiBdatW3fR5a+88gqSk5Px2muvYcuWLYiLi8OTTz6J3/zmNwO6ncDAQBQWFuL3v/89Nm3ahPXr1yM0NBRpaWn47//+b4SFhbni7hDRVZDEhSM9iIiISBE8h0xERKQCDGQiIiIVYCATERGpAAOZiIhIBRjIREREKsBAJiIiUgEGMhERkQowkImIiFSAgUxERKQCDGQiIiIVYCATERGpAAOZiIhIBRjIREREKsBAJiIiUgEGMhERkQowkImIiFSAgUxERKQCDGQiIiIVYCATERGpAAOZiIhIBRjIREREKsBAJiIiUgEGMhERkQowkImIiFSAgUxEXkUIAZPJhO7ubjgcDqXLIbpifkoXQEQ0EGazGd3d3dBoNNDr9dDpdGhtbUVdXR3a2tpgsVgQFRWF1NRUJCYmQpIkSJKkdNlE30kSQgiliyAiuhKyLMNoNOLYsWNobGyERqOBJEnQaDQQQsBqtUKWZcTExCAgIAB1dXWQZRl6vR6ZmZkYO3Ys/PzYDiF1YiATkUdwOBw4deoUduzYgVGjRiEmJgbR0dGQZRlWqxV9fX2IiopCUlISDAYDAMBms6GqqgpVVVVoampCT08PZs6ciaysLGg0PGNH6sJAJiLVE0Kgu7sbf/7znzFv3jyMHz8ewcHBV9wFLcsympubUV1djQMHDiAmJga33HILwsLC3Fw50ZXjV0QiUj0hBIqKihAREYHc3FyEhIRc1flgjUaDuLg45Obm4kc/+hGEEPjf//1fNDQ0uLFqoqvjMYEshIAQArIsg416It/S09ODN954A3fddRe0Wu2AjiFJErRaLWJjY3HHHXcgLS0Nb7/9Nrq6uviZQqrgUYF8/PhxPPnkk+jr61O6HCIaQm+99RbuvfdeBAcHD/pYkiRBp9Phhz/8IQICAvCf//wHZrOZoUyK85hAlmUZZWVlqKqqwp49ezi/kMiH3H333Zg9e7ZLpy1JkoRVq1bh/Pnz2LBhA2w2m8uOTTQQHhPIDocDRqMRt956K8rLy1FXV8dvtEQ+QqfTuWUOsUajwaOPPoojR46gsrLS5ccnuhoeE8iyLKO2thazZs1CcnIyDh48iN7eXqXLIiIP5+fnh2uvvRbnzp1TuhTycR4VyB0dHYiLi0NOTg6MRiPKysogy7LSpRGRhwsLC0N3d7fSZZCP84gla4QQcDgc/aMkhw0bhtzcXHz66acYNWoUoqOjlS7xitjtdjQ3N8NoNMJiscBiscBqtV709zcv7+vrgxACoaGhyM7ORlZWFiIjI7kEIJELhYaGcgoUKc4jAhkAGhoaEB8fD+CrwRgTJkzA4cOHceTIEcybN0/Vy+EJIXDq1Cm89tpr8PPzw4gRI6DX66HX62EwGODv74+QkBBER0dfdLler4efnx80Gg3a29tRUlKCbdu2wWKxYOzYscjLy0NaWhp0Op3Sd5HIo4WGhuL06dNKl0E+Tr0p9g0VFRVITU3t/7+/vz9WrFiBRx99FNnZ2YiJiVFVq9E54Oz06dP461//ioCAANx7771ITEzsX3/XWe/l/u00YsQIpKenw263w2Qy4cSJE9i4cSPKy8sRFRWFWbNmIS8vD9HR0Rf9npoeEyK14gBRUgOPWDpTCIEXX3wReXl5yMnJuejy/fv3Y+vWrfjtb3+reCvZuXiJzWbDmTNnsH79elgsFtx///1ITU112W4zzqfM+XdjYyP27t2Lffv24fz58xg/fjxmz56N9PR0GAwG+Pn5wc/Pj+FM9B22bduG1tZWrFy5UulSyId5TCA/8MADePbZZxEZGfmtnz/99NOYNWsW8vPzFQkd50N4/vx51NfXY/fu3WhsbMTSpUuRlZU1pF8ULBYLTp48icLCQpSXlyMsLAwTJkzAjBkzEBcXB71eP2S1EHmK//znPwgJCcEtt9yidCnkwzyiy1oIgfb2dkRERFzy52vWrMHjjz+OtLQ0JCYmDnltLS0tqKmpwcmTJ1FbW4vZs2dj1apVioSfwWBAdnY2srOz+2s7fPgw/va3v/XvcjNixIgBLz9I5I0aGhpw/fXXK10G+TiPaCGbzWasWrUK69evv+TPnQvPf/rpp1i9ejWCgoLcXpMsy2hoaMCJEyfQ1NSE9vZ2ZGdnY+rUqaprhQoh0NXVhS1btsBsNmPs2LGYNGkSd7ohwlefLw899BCef/75S/bAEQ0Vj2gh19bWfm/LNzc3F6dPn8ZHH32EW2+91W1d185WZ1FREdrb2xEQENAfcGod7SxJEsLDw7FixQqUlpaiuLgYVVVVuO6665CamsrWMvm0wsJCZGZmfmcPHNFQ8YiFQSRJwg033HDZn/v7++PGG2/EmTNnUF5e7rZaurq6UFhYCJPJhBkzZmDJkiXIyclRbRhfSKfTISsrC4sXL0ZKSgref/99vPvuu7BYLBxlSj7JZrPhgw8+wJIlS5QuhcgzAjkpKQm5ubmXvY4kSRg+fDiuvfZafPzxxzCbzS6vQ5ZlVFRUoL6+HgsXLkRGRgYMBoNHjV6WJAlRUVGYMWMG7rzzTlgsFjz++OOoqKhQujSiIbd//36kp6cjNjZW6VKIPCOQ/f39YTAYvvd6Wq0WEydORFBQEA4fPuzSVp8QAh0dHSgqKsLMmTMRFhbmUUH8TTqdDiNGjMDSpUvx0EMP4cUXX8Rf//pXWK1WtpbJJwghsHXrVsyfP5/TAkkVPCKQr5QkSYiIiEB0dDRaWlpctkWjEAKyLOPkyZOwWq249tprveLNK0kSDAYDUlNT8ec//xnR0dFYvXo1Tpw4AbvdzmAmryWEwIkTJzB8+HCPWXqXvJ9XBTLwVcikpqbCZDKhsbHRZcft6OjAhg0bcM8993hFGF9IkiTo9XrcfvvteOSRR/DWW29h48aNaG9v5+Yd5HWEEOju7sbWrVsxdepUBAcHe917mjyT1wUy8NU5Z4vFgqamJpe08hwOB95++23k5+cjJibGBRWqk0ajQXp6Oh5//HFYrVa8+eabaG1tZUuZvIrZbMa2bduQlJSE8ePHc5YBqYZXBrJer8ewYcPQ0tICq9U66OMdPXoUtbW1PrOKT3h4OH784x8jPDwcO3bsgNlsZiiTV+jr68Phw4fR3d2NvLw8BAcHK10SUT+vDGQAmDx5MsrKytDZ2Tmo47S3t+PNN9/EqlWrFF8reyj5+fnh1ltv7e/aYyCTp3POkvjiiy9w/fXX9+8eR6QWXhvI8fHxsFqtMBqNAz4PKoTAf/7zH+Tl5SE5OdnFFapfcHAwbr/9dpw8eRJFRUVKl0M0YEIItLW14d1338X06dORnp7O88akOl4byFqtFrm5ufj8889hs9kGdIySkhKcP38ec+bMcXF1niM2Nhb33HMP3n77bVRVVSldDtGA2O12/OUvf8HUqVORnZ0NjcZrP/rIg3n1q3LatGk4fPjwgALZarXivffew4IFCxAaGurT36YTEhLw0EMP4bnnnkN3d7fS5RBdtZdffhlJSUmYNWuWT516Is/i1YEcEBCA2NhYnDlz5qrOgQoh8NFHH2HMmDFITk726TB2SktLw7Jly/DCCy/AbrcrXQ7RFRFCoLCwEKdPn8bKlSs9Yolb8l1eHciSJGHRokXYsmXLFQeyEALNzc2orq5GZmYmgoKCfD6QJUmCVqvF1KlTkZSUhM2bNzOUSfWEEKiqqsK6devwu9/9ji1jUj2vDmQAyMzMRFlZGUwm0xVd32az4cCBA4iLi0NaWprPh/GFgoODkZ+fj8bGRpSUlLhsJTQiVxNCoKmpCa+++irWrFmDkJAQvpdJ9bw+kDUaDWbNmoW9e/de0fXr6+vR0NCASZMmqW5fYzVISEjAtGnTcODAAdTX13M6FKlSR0cHtmzZgpkzZyI1NZVhTB7B6wMZABYsWICCgoIrmv7kbPVdyWYWvkij0WDSpEkYPXo0du/ejc7OToYyqYrRaMRHH32EiIgITJ06Ff7+/kqXRHRFfCKQ4+LiEBAQgNra2u+9bkJCAoYNG4aSkhKeJ/0O/v7+mDlzJiRJwp49ewY8rYzIlZw7shUUFMDhcCA/Px9hYWFKl0V0xXwikCVJwqRJk1BaWvq91zUYDMjMzERDQwPq6uqGoDrPFBISgoULF+LEiRMoLi5mK5kUJYSAyWTCG2+8Aa1Wi5tvvhkxMTHsqiaP4hOBDACJiYk4d+7c915PkiSkpKRAp9OhsrKSrb/LiI6Oxs9+9jOsXbsWDQ0NSpdDPsxut+P5559HREQEbr31VkRGRjKMyeP4TCDHx8ejqanpiq7r5+eHefPmobi4mLsdXYYkSRg+fDgee+wx/OEPf1C6HPJRNpsNa9asQXp6Om6//XYEBQUpXRLRgPhMIMfFxaGlpeWKr5+QkIDw8HCcPHmS03suQ5Ikdg2SotauXYvZs2dj+fLlXPiDPJpPBLIkSdDpdFfV/SxJEpYtW4Zt27aho6ODreTLaGlpQWxsrNJlkI/6+c9/jkWLFvFLIXk8nwjkC11NsEZFRSEnJ4fbD36P5uZmDBs2TOkyyEdptVqGMXkFnwpkvV4Pq9V6Vb+zePFifP7556ipqXFTVZ6PgUxENHg+E8gajQZRUVFoa2u7qt8LDAzEnXfeiVdffXXA+yp7OwYyEdHg+UwgOwcfXc3ALqecnBz4+/vjwIEDbqjM8zGQiYgGz2cCWaPRICYmBq2trVf9u1qtFitXrsSbb77JecmXwEFdRESD51OBHBUVNaBABr6axzx58mRs376dA7y+wWazcSMOIqJB8plAliQJ0dHRAwpk57Sp/Px87N+/Hz09PW6o0HMJITjKlYhokHwqkMPCwtDZ2Tng34+JiUFubi62bdvGxUK+Jssyw5iIyAV8KpA1Gs2gupsDAgKQmZmJlpYW1NTUsOsagMlk4lKFREQu4DOBfKHBTF8aNWoURo8ejSNHjsBsNruwKs/U1taGmJgYpcsgIvJ4PhXIWq0WBoNhUEHq7++Pa6+9FkajEZWVlT7fSm5ra0N0dDS7rYmIBsmnAtnPzw8hISHo6OgY1HHi4uKQnp6O4uJidHV1uag6z1RXV8cWMhGRC/hUIOt0OpcEskajwdSpU9Hc3IyqqiqfXcHr8OHD+OyzzzB16lS2kImIBsmnAtlVLWTgqwFeS5cuxVtvveVz55KFENizZw+2bt2Ke+65B6NGjVK6JCIij+dTgeyqFjLw1ajtlJQUJCYmYteuXT5xLlkIAbvdjl27dqGoqAgrV65Eeno6NBoNW8hERIPkU4Gs1WoRFBSE7u5ulxxPkiTce++92LBhA4xGo0uOqVZCCFgsFnz88cc4cOAA7rrrLiQnJ0OSJIYxEZEL+FwgGwwGmEwmlx0zICAAq1atwt///nf09fW57LhqIoRAV1cXtm/fjsOHD+P+++9HYmKi0mUREXkVnwpkZ2vO1YOw8vLyIITAZ5995tLjqkVdXR3eeecd1NXV4ZFHHuFGEkREbuBTgQzALV2skiTh7rvvxo4dO9Dc3OzSYytJCIGjR49i8+bNCA0NxQMPPICQkBClyyIi8ko+GcgAXLoWtSRJiI+PR15eHt59912vmAblcDiwa9cuFBYWIjc3F0uWLIFOp1O6LCIir+VzgazT6aDVamGxWFx6XK1WiylTpiAgIACNjY0uPfZQ6+vrwwcffICysjLcdNNNmDp1KjQan3upEBENKT+lCxhq/v7+8PPzc/mmCJIkITIyEkuXLoW/v7/LjjvULBYLCgoKUFtbi2XLlmH48OEMYyKiIeBzgazX6/sD2dU0Go3H7nzknNb0wQcfoLKyEvfffz8iIiI4pYmIaIj4XCA7W8i9vb1Kl6Iasiyjp6cHmzZtQmNjI9asWQO9Xs8wJiIaQj4XyO5sIXsSu92O7u5udHd3o729Hbt27UJQUBCeeOIJ+Pn53MuCiEhxPvfJ66stZCEEZFmG0WhEY2MjjEYj6uvrUVtbi8DAQMyePRs5OTlsFRMRKcTnAtnXWsiyLKOpqQnl5eVoa2uD1WqFyWSCXq9Heno6lixZgoCAAAYxEZHCfC6QtVotNBoNbDab0qW4jSzLaGxsxNGjR1FbWwu9Xo+wsDAEBgYiJSUFycnJCA0NZQgTEamIzwWyt4aQEAKdnZ0oLCzEwYMHYbfbkZycjJiYGISGhiI6Ohp6vR52ux3l5eWw2+2w2+0ICAhAcnIyR1QTESnM5wLZSQgBIYTXhJBzZa1PPvkEVqsVer0e9fX16Ozs7F8Mxc/P71t/ent7sXHjRnR1dSEwMBBjx47FhAkTkJ6ejpCQEK95fIiI1E4SvrCR7zfs2bMHnZ2dmD9/PvR6vdLluIQQAjabDXa7HcDFPQGX+7cQAg6Ho3/q08mTJ/Hll1+itLQU58+fx4gRIzBhwgRMmjQJY8aMgU6n+87jERHRwPlkIH/xxRcoLy/H3LlzERUVpXQ5qvDNl4GzB6GhoQFffvkljh49irKyMgBAWloaFixYgPT0dBgMBmg0GgYzEdEg+WQg19TU4NNPP8V1112HpKQkpcvxKDabDaWlpXj//fdhNpsxb948XHPNNYiIiEBQUBCDmYhogHzyHHJERARsNht6enqULsXj6HQ6TJgwAVlZWairq8P27duxd+9ejBs3DqmpqYiLi8OwYcO4uAgR0VXyyU/N4OBg2O12l+/45EskSUJiYiJWrVqFnp4efPbZZygqKkJYWBiioqKQlJSE1NRUGAwGtpqJiK6AT3ZZA8Arr7yCrKwsrk7lIs5BZeXl5SgvL0d3dzesVitSUlIwdepUdmcTEX0Pnw3kDRs2YNiwYZgxYwZ0Op3S5XgVu92O5uZm1NXVoaamBhUVFYiNjcVNN92EkSNHKl0eEZEq+Wwg7927Fx0dHZg3bx6Cg4OVLscrCSFw/vx5GI1GVFdXY8+ePRg1ahRWrFgBg8GgdHlERKriszvPx8XFoaOjA1arVelSvJYkSQgNDUVSUhKuv/56rF69GjqdDs8++yzq6ur6p1YREZEPB/KwYcMYyENEkiT4+fkhKioKd955J/Lz8/HSSy/1L/HJUCYi8tFR1gAQHh6Onp4er95kQm0kSYJOp8OsWbOQmJiIl19+GTU1NcjPz0d0dDQHfRGRT/PZFrIkSdBoNOw2VUhycjJ+/etfo6urC6+88gpKSkp8ZktMIqJL8dkWMgCEhoaiq6vLqzaZ8CQBAQFYtWoVjh49ioKCAlRWVmLy5MlISkri80FEPsdnW8gAkJiYiPr6ejgcDqVL8WmTJk3Cww8/DIfDgR07dmDv3r0wm81Kl0VENKR8OpBHjRqFuro6BrLCnKOxf/jDH2LatGkoLy/Hhg0b0NLSwtMJROQzGMgMZFVwDvjKysrCbbfdhoiICPz9739HaWkpZFlWujwiIrfz6UCOiIjoP4dM6qDRaBAdHY0f/OAHWLZsGf7v//4Pe/fuZSh7uZKSEr4Pyef5dCA7R1mTukiSBL1ej4yMDDz77LP45z//iY6ODqXLIjeQZRmbN2/G5s2blS6FSHE+HcikbpIkISQkBI8++iieffZZ9PX1KV0SuZDD4cCBAwdQWFiIp556SulyiBTHQCZVkyQJOTk5GDZsGLZv386uay/g3Bns+PHjKCgowJNPPgm9Xs+pbuTzGMjkEX7+85/j008/RVlZmdKl0ADJsgyj0YijR49iz5492LlzJ+644w4MGzaMYUwEH18YhDyDs+v69ttvx/vvv4+4uDhEREQoXRZdBSEEmpubsWXLFhgMBgQGBmLRokVITk6GRsN2ARHAQCYPodFoMG7cOFRVVWHnzp1YsmQJtFqt0mXRFTKbzXj11VeRmZmJWbNmITQ0lK1iom/gV1PyGAEBAZg+fTqMRiOOHTumdDl0hYQQeOmll5Camoof/OAHCAsLYxgTXQIDmTyGJEkYMWIE0tLScPToUXR1dSldEl2B9evXIyAgAIsXL4Zer1e6HCLVYiCTR9FqtZg2bRosFgu++OILjrpWKSEEZFlGQUEBjh49ivvuu49hPAjOXeku/EPeh+eQyeMEBgZi1qxZ2LlzJ1JSUpCQkMAuUBURQqCnpwdvvPEGjEYjfvvb3yIwMJDP0VVwfqGx2Wyw2Wyw2+0oKyuD2WxGcnIywsPDodfr4e/vz0FxXoSBTB5HkiSMHTsWhw4dwqFDhxAbGwt/f39+4KuAEAJNTU3YsGEDIiIisGLFCoSEhChdlkdxOBzo7OxEeXk5jh07hlOnTqGvrw9jxoxBUFAQdu/eDaPRiIyMDNx2220YMWIEQ9lLMJDJI0mShGXLluGPf/wjUlJSMHHiRKVLIgA9PT3YvXs34uPjsWjRIhgMBqVL8hhCCJw/fx6lpaU4evQo7HY7pkyZghUrVnyrh8Fut+P9999HQUEBFixYwF4iL8GvVeSxgoODsXz5cmzcuBFGo1HpcnyeLMuoqqqC0WjEjTfeyDC+CkIIHDt2DFu2bEFJSQmys7Px85//HFOmTEFQUNC3wtbPzw+LFi1CbGws3nnnHdTX1/O8shdgIJNHGzduHLKzs/Hvf/+bA7wU1tvbi8LCQuTm5iI8PFzpcjzKvn37sHPnTsTHx+P222/HlClT4O/vf9nfcYZyWloa1q1bN0SVkjsxkMmjSZKEBQsWwGazYffu3UqX47NkWcaxY8dgsViQnZ2tdDkepaioCDt37sTixYsxb968q1qFzs/PDzfeeCPq6urQ2NjoxippKDCQyePp9XrcfffdeO+999Dc3Kx0OT7JaDTiX//6F+68805Ob7oKxcXF2LJlC+677z6kpqYO6Dywn58fli5dio0bN7qhQhpKDGTyCrGxsfjRj36Ef//730qX4lOEEGhvb8dDDz2EBx98EAkJCUqX5BGEECgrK8Nbb72F+++/H4mJiYM63nXXXYeDBw/CZrO5qEJSAgOZPJ4kSZAkCampqVy9awgJIdDS0oLVq1fjySefxMSJEznS9woIIWA0GrFx40bceeedSE5OBoBBPXb+/v6YM2cOPv74Y1eVSQpgIJNXYBAMLSEEzp07h+effx4PP/wwMjMzXfocePNqVFarFdu3b8fYsWMxfvx4aDSaQT92kiTh5ptvxq5du2C3211UKQ01BjIRXbXe3l5s2LABt9xyCyZNmuTyhSmqq6u9cjyA3W7H559/jt7eXkyfPt1l59slSUJ4eDhiYmJQWVnpkmPS0GMgw7u/jRO5mhACR44cQUxMDLKysuDn59r1hYQQeP3117Fr1y6XHldpQgjU19ejuLgYubm5iI+Pd+nxDQYDrrvuOnzyySf8PPNQPh/IOp2OXTxEV6G1tRVVVVXIyMhwy3xjq9UKk8nk8uMqSQgBk8mErVu3IiUlBRMmTHD5bWi1WiQmJqK3txdtbW0uPz65n88vnRkYGAiTycSFDLyIEILnlN3E4XDg1KlT0Gq1yMjIcMvj3NzcjDNnzmDcuHEuP7ZShBD4+OOPIYTAnDlzoNVqXX4bkiQhIiICo0ePxrFjx5Cfn+/y2xgoh8OBnTt34siRI99a8ESWZfj7+yMvLw+TJk2CTqdTqErl+XwgBwUFoaenR+kyyAUkSYJWq4XdbvfpN7U7tba24vTp0/1LOrrDiRMn0NHRgdTUVLccXwl1dXXYtm0bnn32WQQGBrrtdoKCghATE4OqqirVfDE9fvw4XnnlFWRlZWHp0qXfem861/A+dOgQNmzYgPz8fMybN88n38M+H8iBgYHo7e1VugxyAUmSoNPp0NfX55NvZnez2Wz48ssv0dvbi/Hjx7vlw95ut6OpqQnR0dEYOXKky4+vBIfDgV//+tf45S9/icjISLfelkajQWRkZP+a4tHR0W69vUtxnr82Go1Yu3Yturu78eCDD2LEiBEICAi45OtGlmWMGTMGRqMR7733Hvbv34+f/vSnLpkS5kl8/hxyUFAQA9lLXBjI5Fp2ux0ff/wxduzYgXvuucctXa7Oec3Hjh1DWlra967l7AlkWcarr76K2bNnu62L/5tGjBgBnU6HM2fODPngLiEEHA4HCgsL8Zvf/AbZ2dn4f//v/yE1NfU7wxj46otEYGAgRo4cifvvvx8LFy7Eiy++iO3bt6Ovr89nBqn5fAuZgew9NBoNA9kNLBYLduzYgT179uCPf/yj27pcnYFsNBqRk5Pj8b0cQgicPHkSpaWleO6554bsdsPDwxEQEIBz584N6TlZ58C1Tz75BF988QUee+wxJCUlXfHvO8Nap9NhypQpGDNmDJ5//vn+88s6nc7rW8o+30Jml7X3YAvZ9To7O/Huu++iuLgY//M//+PW8599fX2oqKjo7w539XSqodbW1oa3334bDzzwAPz9/YcsTCRJwoQJE9DS0oKGhoYhaV0KIdDb24sPP/wQJ0+exH333XdVYXwp4eHhWLNmDXbt2oU9e/b4xPva5wM5ODiYgewlGMiuZbFY8O6778JoNOK//uu/3N6F7HA40NnZiaioKADw6EDu7u5GQUEBpk6disTExCFv2Y0ePRoAUFtbC4fD4fbbk2UZBQUFaGhowIMPPohhw4a55LiRkZF44oknsH//fuzatcvru659PpA5ytp7MJBdy7mi1MqVK4ekhedwONDV1YWwsDAAnjuQx2q1Yv/+/RBCYPr06TAYDENeg1arxfTp0/HFF1/g/Pnzbr+9Tz75BOXl5bj77rtdPvo+IiICDz/8MD777DMcO3bMpcdWGwby1+eQvf2bly/gOWTXMZlMOHLkCGbMmIHg4GC3h6MQAn19fWhqakJMTIxbb8udZFnGyZMnUV5ejtmzZ7t9VPXlpKeno7W1FZ2dnW79fCsvL8dHH32Eu+66y23rOURFRWH58uXYvHkzjEajW25DDRjIbCF7DZvNhra2NgQEBChdiscrLi5GZGQkRo8e7fJ1qi9FCIEvv/wSycnJHvv8CSHQ2NiIjz/+GNddd50iXdUX0ul0yMzMxPHjx93WbS3LMl5++WXccccdbp+mNnbsWOTm5mLTpk1e24Dy+UDmoC7vIMsySktLYbfb+8+f0cCYTCZUVlYiMTGxv/vY3WRZxubNm3Hrrbf2b7jgaT0dZrMZW7ZsQVpaGrKystwyNexq5eXloaioyG2B/Omnn2L48OEYO3asW47v5BzkN2nSJFgsFlRXV7v19pTi84EcHBw8JOdYyH2cIzxfeeUV3HvvvR4/XUZJQgicOHECDocDmZmZQ9I6BoCamhro9XrExcUhKCgIkiR51PvS4XBg9+7d6Orqwvz581UzIG3YsGFoa2uDzWZzeauyr68PH374IRYtWgS9Xu/23gBJkhAbG4uxY8eisLDQK1vJDGSOsvZ4sixj7dq1uO222/pH6NLA9Pb24uzZs4iLixuy859CCLz55ptYvnw5JElCSEgIJEnymFNJQggcP34c77//Ph577DFVtIydJEnC5MmTUVxc7NLjyrKMHTt2YOLEiS4bUX0l/Pz8kJCQ0D/ewNv4fCBrNBrIsqx0GTQIJSUlqK6uxsKFCz12ZK4ayLKM6upq1NXVIS8vb8geS5PJhJMnTyInJwfAV1+Sga+mDqmdEAIVFRX429/+ht///vdDOt/4SuXm5uLw4cMuO54QAufOnUNFRQVycnIQGBg4pHOsk5OTERYWhqNHj3rdTn0+H8jk2VpbW7Fu3To89thjqvsg9DQWiwXFxcUYN27ckJ07BoDdu3dj7ty5/c9fYGAgNBoNjEajqj9whRAoLy/Ha6+9htWrVyM2Nlbpki4pPT0dpaWlLjuexWJBUVERrrnmGiQlJQ35+85gMGDcuHGoqalBbW2tV3VdM5DJY9ntdmzatAlz5sxBQkKC0uV4NOeyh8ePH8fMmTOH7Hbtdjt27tyJ+fPn91+m1WqRlpaGhoYGtLe3D1ktV+vMmTP44IMPsGDBAowbN061XwiDg4PR19cHm8026GPJsozy8nJ0dnbi2muv7R+AN9QyMjIQHx+PAwcOeNXe2QxksNvaU+3fvx99fX2YM2eOaj8MPYnVakVAQIBbl8f8prKyMsTGxiI6Ovqi5zAjIwPd3d04d+6cKt+b586dw0cffYSJEydi8uTJQzb4baBGjx6Ns2fPDvo458+fx/Hjx5GWloYRI0a4oLKB0Wq1mD17NhoaGlBaWqrK18hAqPtVNES4wYTnaWtrw6FDhzBnzpwhWbjCF3R1dSE0NHRIb3Pr1q246aabvjUQymAwYNKkSSgpKYHZbB7Smr5Pc3MzPvjgA8THx2P69OkeMao/Jydn0AO7nJt/1NTUYMqUKYq/58LDw3HLLbdg48aNMJvNXtF1zUDGV09sZ2en0mXQFZJlGbt27UJycjKuueYa1bdOPIEQAt3d3UN67ri0tBRWqxUpKSmX/HDPzc1FZWUlLBbLkNV0OUIINDc3Y/369YiOjsYNN9wwpAOaBsMVI62tVisOHjyISZMmuXx5zIG65pprkJiYiKKiIqVLcQl+kuGrtVI7OjqULoOugBACZWVl6OzsxMSJExVZJ9hbDWUL2Wq14r333sMNN9yAiIiIS4aawWBAX18fZFlWvPUjhEB7ezteeOEFjBkzBosXL/aYMAaAuLg4tLS0DPhxdM71/+yzz5Cfn6+a+63RaPCjH/0I69evV/w14goMZLCF7EnMZjNKSkoQGxuL5ORk1XwweDpnC3koAlmWZezevRsjR45ERkbGdz6HztWZlB5pLcsyGhsbsWbNGsyfPx8LFy70uL15JUlCYGDggOd2y7KMt99+GzfddJPbd/26WhEREUhLS8PBgweVLmXQGMhgIHsKWZZx+vRpNDQ0cCCXGwxFIAshUF9fj5qaGmRmZiI0NPSyz2NSUhIqKysVa/3Y7XacOnUKTz/9NH7xi19g5syZHvm6kyQJmZmZOHHixIB+v6qqCiUlJViwYIHq7r8kSbj33nuxbt06l4wkVxIDGeyy9gRCCHR2duLAgQOYMmUKIiIilC7JqwghLtr60F2c3Z4xMTEYP378914/Ly8Pu3btGpI9fb/JarXi0KFDWLduHX71q19dUb1qJUkSxo8fj1OnTl3175pMJvzjH//AI488oqpVyC4UFxeHuLg4HD9+XOlSBoWBDAay2gkhUF1djYKCAuh0OkybNk3pkryOEAINDQ1uXdzCbrfj+PHjaG1txezZs69ovecxY8ags7MTjY2NQ9pK7unpwZ49e7B//3488MAD3znwzJM417W+GkIIbN26FSkpKW7fQGIwJEnC0qVL8f7773v0uWQGMhjIamYymbB582YUFBQgJCQES5cuVe23dE9ms9lgNBrdthexEAI1NTXYs2cP5syZg+jo6Cv6PY1Gg1tuuQWbNm1yS13f5NxCccuWLairq8OyZcsU30bRVQaybn91dTU+//xzLF++3E1Vuc6oUaNgMplUvZjM92Eg46tzyAxkdRFCoLS0FC+99BKsVivy8/Nxyy23sKvaTUpLSzFmzBi3fdmpra3F2rVrMWvWrKtuaU2fPh1VVVWoqalxS21ODocDJ06cwLp16xAaGopFixZh5MiRXjOt7mrXW3A4HHjnnXdw8803f+dIeLWQJAl6vR6TJ0/GoUOHlC5nwLzjlTZIoaGhHNSlImazGe+88w42b96M/Px8LFq0CBkZGWwZu9Hu3bsxZ84ctxy7trYWzzzzDO644w5MmTLlqgMuMDAQ99xzD/7+97+7pT7gq/DZvHkz3njjDdx888244YYbEBsb6zVhDHy1U9LVDHo6cOAAgoODh3QbzsHQ6XSYMGECjh496rHd1up/lIeATqdTfGqFrxNCQAiBpqYmvPDCC2hubsa9996LyZMn9++PS+4hhMDhw4cxadIklx7Tee7/qaeewpo1a5CVlTXgfYIzMzOh1+td+mHrrLG+vh73338/Ghsb+wdvBQQEeNVrTpKkq7o/vb29OHjwIHJzcxEWFuYxj0VYWBgCAwPR0NCgdCkDoo5dtBV2tS9Wci0hBPr6+nD06FH861//wrJlyzB79mxotVo+L0OgtLQU6enpAw7LCzlDzmw248MPP8SuXbvw1FNPIS0tbVCtLJ1Oh5/+9Kf405/+hLFjxw56QRhZlmE2m7Fx40bs3bsXv/zlL5Genu71nwXO5+dy91EIgX379iE+Ph5paWke83hIkoSIiAiMGTMGBw8exG233eYxtTsxkElRNpsNbW1t2LBhA+rr6/G73/1uSDc8J+DDDz/ETTfdNOjjOIO4oqICW7ZsQVxcHF544QWXtLAkScLw4cORl5eHd999F7fffvuATmE4aywvL8eGDRuQlpaGf/zjH17XIv4uzo10vuuxc65XXV9fj6ysLISEhAxxhYNjMBgQGxuL4uJiWK1Wj1vJj4FMihBCoK2tDSdOnEBBQQHmzJmD1atX+8SHotoYDAacPXsWqampiIuLG9BzYDabcfbsWRw7dgzl5eVYuHAhJk6c6NLz/gaDAdOmTcOGDRtQVlZ22VW+vkkIAZvNhurqahQXF+Ps2bNYvnw5xo8f7zNjEzQaDUJCQnD+/HmEh4df8jp2ux3Hjh2DRqPBxIkTh7Q+V0lISMCpU6dQWVnpcXPHGcg05CwWC06dOoXPP/8cZrMZa9asGXAQ0OA9+OCDeP3117Fp0ybk5eUhKyvriruXLRYLTp48ibNnz6KmpgbJycn4xS9+geDgYLfUGhcXh8mTJ2Pfvn2IiYn51raNl2Kz2VBeXo6KigrU1tZixIgReOihh4Z8ZyulaTQaREZGoq2t7ZKB7Nw8o7S0FDfeeKNiex0PVmxsLEJCQlBdXe1xg0EZyDRknK3iTz75BG1tbRg/fjxycnJgMBgYxgry8/PDypUrUVRUhMLCQrS2tiIvL++yH8jnz5/Hl19+iYqKCtjtdsTExOCnP/0pIiMj3V5rVlYW2tvbsWnTJiQkJCAnJ+eSX+gsFgtKSkpQWloK4KvR2osWLfKaecVXS6PRICIiAkajEampqd/6uc1mw6FDhxAdHY20tDQFKnQNrVaLhIQEVFZWorOzE1FRUUqXdMUYyBeQZdkjhvd7GiEETCYTPvnkE5w4cQJpaWmYP38+EhISPOrbqzfTaDSYMWMGYmNjUVhYiOLiYsTGxiIjIwMZGRn9LarW1lbs3bsXtbW1iI+PR1JSEjIyMjBs2LAhC7nw8HDMmjULFRUVaGxsxIYNGyDLMhITE9He3o7u7u7+kdixsbEYNWoU0tLSkJCQ4JNB7KTRaBAQEHDJ/aWdX5YPHz6Mxx9/3OM/B5OSknDs2DG0t7czkD2Rc6s3TxsEoHZnz57Fli1bcO7cOeTm5mLx4sUYOXKkavZTpf+fVqtFWloaoqOj0dbWBqPRiMrKSuzYsQPd3d3w9/eHTqfD1KlTsXDhQsTGxiIsLGzIP7ydo2lzcnJgsVjQ3NyM1tZWdHR0YNSoUQgODoZGo+nvonUuauHLYewkSdIlp40JIfDmm29i7ty5bu/lGAoRERGwWq0wmUxKl3JVGMhfc35zZCAPns1mQ0lJCd5++21oNBosXrwYY8aMQVBQELunVU6j0SA6OhpRUVFwOByYOHEirFYrHA4HzGYzgoODERQUBH9/f8WfR0mSEBAQgKSkJIwcORIOhwNarRYajUbx2jxNbW0tysrK8PDDD3vFY6fVaqHT6dDe3g673e6SKX1DwTOqHAIGgwFms5lLMw6AEAKyLMNiseDTTz/F5s2bMXz4cNx3331ISkrq3zvWG97ovsK5F7Gfnx+CgoL6W1VqfQ61Wi1PfwyQEAJ/+ctfsHr1atXtdTxQkiThBz/4AV599dUhP6UyGAzkrwUGBsJisShdhkdxOBwwmUzo7u7Gjh07sG/fPuTk5OCZZ57pn0vsCW8C+n58Hr3DpRYF6enpQVNTE8aNG6dQVe6RkJCAESNG4ODBg1iwYIFHLDTEQP7adw12oIsJIWCxWNDW1ob6+nrs378f9fX1mDt3LtauXeuxUyWIvJ0QAlar9Vvv0e3bt2PBggUKVeU+kiRh+fLlePLJJ5GdnY2EhASlS/peDOSvMZAvTwiB9vZ2VFVVobGxEdXV1fDz88ONN97ocXP9iHyRw+FAS0sLrrvuuv7LrFYr9u3bh9/85jcKVuY+ISEhuPXWW/H666/j8ccfV/25ZHVXN4QCAwN9NpCd69tarVaYzeZv/bFYLGhqaoLZbIbD4UBUVBSWLFmC+Ph4j58eQeQrHA4H2traLlqa9tChQ7jmmmsQHh6u+u7cgZo1axYKCgpQWVmJ9PR0pcu5LAby1yIjI2E0GpUuwy1sNhv27duHnp4e9Pb2ore3F1arFbIsX3Q9nU4HrVbbP5hHp9P1/9s5mnXMmDEICwtT6J4Q0UA51/EODAwEAJhMJuzduxdLly5VfctxMLRaLcaPH4+amhoGsqdISkrCgQMHlC7DbXp7exEQEIDIyEgEBARcctrKhSGs0+ku+ndAQIBXv2mJfMGFg7qKioowevRojBw50mtbx05ardYj9kjmJ+zXRo8ejbfeekvpMtzCz88Pc+fO7Z+j6Rxt6O1vQiK6mCRJkGUZkiTh9OnTmD59en+L2ZvFx8d7xJRWBvLXAgMDYTKZvnevUE8kSZJPvOmI6LtpNBqEhYWhs7MTer0eOp2uf1Uzb5efn+8Rn+sMZKC/tej89sgRw0TkbbRaLWJiYtDc3IyQkBAYDAYEBAQoXdaQ8JTTbd7/1egKSZKEmJgYtLa2Kl0KEZHLSZKEkJAQdHV19beSfSWQPQUD+WsajQYJCQmor69XuhQiIpeTZRmdnZ2IjIxER0eHT7WQPQUD+WvOPTRra2uVLoWIyOUcDgeam5sRHx+Prq4u6PV6bqajMgzkrzlbyHV1dUqXQkTkckKI/umP58+f719jgNSDgfw1SZIQHh6Ojo4OpUshInKbhoYGmEwmJCYmesTIY1/CQP6aJEnQarVwOBxKl0JE5BYajQaVlZWQZRkpKSlKl0PfwEC+gEajgV6vh8lkUroUIiKXcq5WtX//fmRnZ0On0yldEn0DA/kC/v7+iIyMRHNzs9KlEBG5lHNhkPb2dkycOJHd1SrEM/oX8Pf3R0REBFpaWjB69GilyyEichmdToebbroJer3eJ1bn8kQM5As4W8hNTU1Kl0JE5FI6nQ7Tpk1Tugy6DH5NuoC/vz/CwsLQ1tamdClERORjGMgXkCQJUVFRiI2NVboUIiLyMZLwhE0ih5DFYoHZbPaIrbqIiMh7MJCJiIhUgF3WREREKsBAJiIiUgEGMhERkQowkImIiFSAgUxERKQCDGQiIiIVYCATERGpAAOZiIhIBRjIREREKsBAJiIiUgEGMhERkQowkImIiFSAgUxERKQCDGQiIiIVYCATERGpAAOZiIhIBRjIREREKsBAJiIiUgEGMhERkQowkImIiFSAgUxERKQCDGQiIiIVYCATERGpAAOZiIhIBRjIREREKsBAJiIiUgEGMhERkQowkImIiFTg/wMHc5lt5sp3PAAAAABJRU5ErkJggg==\n"},"metadata":{}},{"name":"stdout","text":"Done. Wrote 1 predictions to: pred_one.json\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"!zip -r outputs.zip outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T19:03:39.925897Z","iopub.execute_input":"2025-12-13T19:03:39.926624Z","iopub.status.idle":"2025-12-13T19:04:43.582610Z","shell.execute_reply.started":"2025-12-13T19:03:39.926599Z","shell.execute_reply":"2025-12-13T19:04:43.581660Z"}},"outputs":[{"name":"stdout","text":"  adding: outputs/ (stored 0%)\n  adding: outputs/images/ (stored 0%)\n  adding: outputs/handwriting_lora/ (stored 0%)\n  adding: outputs/handwriting_lora/checkpoint-1000/ (stored 0%)\n  adding: outputs/handwriting_lora/checkpoint-1000/tokenizer.json (deflated 79%)\n  adding: outputs/handwriting_lora/checkpoint-1000/tokenizer_config.json (deflated 97%)\n  adding: outputs/handwriting_lora/checkpoint-1000/adapter_config.json (deflated 56%)\n  adding: outputs/handwriting_lora/checkpoint-1000/scheduler.pt (deflated 56%)\n  adding: outputs/handwriting_lora/checkpoint-1000/rng_state.pth (deflated 25%)\n  adding: outputs/handwriting_lora/checkpoint-1000/special_tokens_map.json (deflated 67%)\n  adding: outputs/handwriting_lora/checkpoint-1000/trainer_state.json (deflated 78%)\n  adding: outputs/handwriting_lora/checkpoint-1000/README.md (deflated 65%)\n  adding: outputs/handwriting_lora/checkpoint-1000/adapter_model.safetensors (deflated 8%)\n  adding: outputs/handwriting_lora/checkpoint-1000/optimizer.pt (deflated 13%)\n  adding: outputs/handwriting_lora/checkpoint-1000/scaler.pt (deflated 60%)\n  adding: outputs/handwriting_lora/checkpoint-1000/training_args.bin (deflated 52%)\n  adding: outputs/handwriting_lora/checkpoint-801/ (stored 0%)\n  adding: outputs/handwriting_lora/checkpoint-801/tokenizer.json (deflated 79%)\n  adding: outputs/handwriting_lora/checkpoint-801/tokenizer_config.json (deflated 97%)\n  adding: outputs/handwriting_lora/checkpoint-801/adapter_config.json (deflated 56%)\n  adding: outputs/handwriting_lora/checkpoint-801/scheduler.pt (deflated 56%)\n  adding: outputs/handwriting_lora/checkpoint-801/rng_state.pth (deflated 25%)\n  adding: outputs/handwriting_lora/checkpoint-801/special_tokens_map.json (deflated 67%)\n  adding: outputs/handwriting_lora/checkpoint-801/trainer_state.json (deflated 78%)\n  adding: outputs/handwriting_lora/checkpoint-801/README.md (deflated 65%)\n  adding: outputs/handwriting_lora/checkpoint-801/adapter_model.safetensors (deflated 8%)\n  adding: outputs/handwriting_lora/checkpoint-801/optimizer.pt (deflated 14%)\n  adding: outputs/handwriting_lora/checkpoint-801/scaler.pt (deflated 60%)\n  adding: outputs/handwriting_lora/checkpoint-801/training_args.bin (deflated 52%)\n  adding: outputs/handwriting_lora/lora_model/ (stored 0%)\n  adding: outputs/handwriting_lora/lora_model/tokenizer.json (deflated 79%)\n  adding: outputs/handwriting_lora/lora_model/tokenizer_config.json (deflated 97%)\n  adding: outputs/handwriting_lora/lora_model/adapter_config.json (deflated 56%)\n  adding: outputs/handwriting_lora/lora_model/special_tokens_map.json (deflated 67%)\n  adding: outputs/handwriting_lora/lora_model/README.md (deflated 65%)\n  adding: outputs/handwriting_lora/lora_model/adapter_model.safetensors (deflated 8%)\n  adding: outputs/handwriting_lora/eval_report.json (deflated 50%)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import hashlib, torch\n\ndef lora_fingerprint(model):\n    h = hashlib.md5()\n    cnt = 0\n    for n, p in model.named_parameters():\n        if \"lora\" in n:\n            h.update(p.detach().cpu().float().numpy().tobytes())\n            cnt += 1\n    return cnt, h.hexdigest()\nmodel, tokenizer = FastVisionModel.from_pretrained(\n        \"unsloth/DeepSeek-OCR\",\n        load_in_4bit=False,\n        auto_model=AutoModel,\n        trust_remote_code=True,\n        unsloth_force_compile=True,\n)\n# trước train\ncnt0, fp0 = lora_fingerprint(model)\nprint(\"before:\", cnt0, fp0)\n\n# sau train\nmodel2 = PeftModel.from_pretrained(model, \"outputs/handwriting_lora/lora_model\")\ncnt1, fp1 = lora_fingerprint(model2)\nprint(\"after :\", cnt1, fp1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T19:12:14.465597Z","iopub.execute_input":"2025-12-13T19:12:14.466189Z","iopub.status.idle":"2025-12-13T19:12:36.513473Z","shell.execute_reply.started":"2025-12-13T19:12:14.466165Z","shell.execute_reply":"2025-12-13T19:12:36.512567Z"}},"outputs":[{"name":"stderr","text":"You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: WARNING `trust_remote_code` is True.\nAre you certain you want to do remote code execution?\n==((====))==  Unsloth 2025.12.5: Fast Deepseekocr patching. Transformers: 4.56.2.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nUnsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n","output_type":"stream"},{"name":"stderr","text":"You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\nYou are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\nSome weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at unsloth/DeepSeek-OCR and are newly initialized: ['model.vision_model.embeddings.position_ids']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"before: 0 d41d8cd98f00b204e9800998ecf8427e\nafter : 4392 d07f00e8d780c80c204da2e90c5622bc\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# Eval 2","metadata":{}},{"cell_type":"code","source":"def normalize_text(text):\n    # Chuyển về chữ thường và xóa khoảng trắng đầu đuôi\n    return text.lower().strip()\n\ndef calculate_accuracy(predictions, ground_truths):\n    correct_count = 0\n    total_count = len(predictions)\n\n    for pred, gt in zip(predictions, ground_truths):\n        # Chuẩn hóa trước khi so sánh\n        clean_pred = normalize_text(pred)\n        clean_gt = normalize_text(gt)\n        \n        if clean_pred == clean_gt:\n            correct_count += 1\n        else:\n            # In ra vài mẫu sai để debug\n            if total_count < 5: \n                print(f\"Sai: GT='{clean_gt}' vs Pred='{clean_pred}'\")\n\n    accuracy = correct_count / total_count\n    return accuracy\n\n# Ví dụ áp dụng\n# Thay vì nhìn WER=1, hãy nhìn Accuracy. \n# Nếu Accuracy = 0 nghĩa là code xử lý chuỗi đang có vấn đề lớn.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T05:01:20.009703Z","iopub.execute_input":"2025-12-14T05:01:20.010372Z","iopub.status.idle":"2025-12-14T05:01:20.015924Z","shell.execute_reply.started":"2025-12-14T05:01:20.010345Z","shell.execute_reply":"2025-12-14T05:01:20.014904Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from unsloth import FastVisionModel\nfrom transformers import AutoModel\nfrom peft import PeftModel\n\nBASE_ID = \"unsloth/DeepSeek-OCR\"\nLORA_DIR = \"/kaggle/working/outputs_finetune/outputs/handwriting_lora/lora_model\"\n\nmodel, tokenizer = FastVisionModel.from_pretrained(\n    BASE_ID,\n    load_in_4bit=True,\n    auto_model=AutoModel,\n    trust_remote_code=True,\n    unsloth_force_compile=True,\n    device_map={\"\": 0},   # ép tất cả lên GPU0, không offload CPU/disk\n)\n\nmodel = PeftModel.from_pretrained(model, LORA_DIR)\nFastVisionModel.for_inference(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T05:01:22.106045Z","iopub.execute_input":"2025-12-14T05:01:22.106730Z","iopub.status.idle":"2025-12-14T05:02:31.606237Z","shell.execute_reply.started":"2025-12-14T05:01:22.106699Z","shell.execute_reply":"2025-12-14T05:02:31.605497Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/1470460709.py:1: UserWarning: WARNING: Unsloth should be imported before [transformers, peft] to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n\nPlease restructure your imports with 'import unsloth' at the top of your file.\n  from unsloth import FastVisionModel\n","output_type":"stream"},{"name":"stdout","text":"🦥 Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/unsloth/DeepSeek-OCR:\n- conversation.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/unsloth/DeepSeek-OCR:\n- deepencoder.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/unsloth/DeepSeek-OCR:\n- modeling_deepseekocr.py\n- conversation.py\n- deepencoder.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nYou are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: WARNING `trust_remote_code` is True.\nAre you certain you want to do remote code execution?\n==((====))==  Unsloth 2025.12.5: Fast Deepseekocr patching. Transformers: 4.56.2.\n   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"name":"stderr","text":"You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\nYou are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30f657290ac4483aa7beece908389364"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-000001.safetensors:   0%|          | 0.00/6.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca2bc50034c2443daeab21018599afba"}},"metadata":{}},{"name":"stderr","text":"Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at unsloth/DeepSeek-OCR and are newly initialized: ['model.vision_model.embeddings.position_ids']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d79328a99d7345fc81ad522ef9ea38e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bf8a0970a2e494eb5abb2d4075e78cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/801 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"822623e5e64c47858eaaf1399ee50ed9"}},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): DeepseekOCRForCausalLM(\n      (model): DeepseekOCRModel(\n        (embed_tokens): Embedding(129280, 1280)\n        (layers): ModuleList(\n          (0): DeepseekV2DecoderLayer(\n            (self_attn): LlamaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n            )\n            (mlp): DeepseekV2MLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1280, out_features=6848, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=6848, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1280, out_features=6848, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=6848, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=6848, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=6848, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): DeepseekV2RMSNorm((1280,), eps=1e-06)\n            (post_attention_layernorm): DeepseekV2RMSNorm((1280,), eps=1e-06)\n          )\n          (1-11): 11 x DeepseekV2DecoderLayer(\n            (self_attn): LlamaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n            )\n            (mlp): DeepseekV2MoE(\n              (experts): ModuleList(\n                (0-63): 64 x DeepseekV2MLP(\n                  (gate_proj): lora.Linear4bit(\n                    (base_layer): Linear4bit(in_features=1280, out_features=896, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Identity()\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1280, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=896, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (up_proj): lora.Linear4bit(\n                    (base_layer): Linear4bit(in_features=1280, out_features=896, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Identity()\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1280, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=896, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (down_proj): lora.Linear4bit(\n                    (base_layer): Linear4bit(in_features=896, out_features=1280, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Identity()\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=896, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=1280, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (act_fn): SiLU()\n                )\n              )\n              (gate): DeepseekV2MoEGate()\n              (shared_experts): DeepseekV2MLP(\n                (gate_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=1280, out_features=1792, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1792, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (up_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=1280, out_features=1792, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1792, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (down_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=1792, out_features=1280, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1792, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act_fn): SiLU()\n              )\n            )\n            (input_layernorm): DeepseekV2RMSNorm((1280,), eps=1e-06)\n            (post_attention_layernorm): DeepseekV2RMSNorm((1280,), eps=1e-06)\n          )\n        )\n        (norm): DeepseekV2RMSNorm((1280,), eps=1e-06)\n        (rotary_emb): LlamaRotaryEmbedding()\n        (sam_model): ImageEncoderViT(\n          (patch_embed): PatchEmbed(\n            (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n          )\n          (blocks): ModuleList(\n            (0-11): 12 x Block(\n              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (attn): Attention(\n                (qkv): Linear4bit(in_features=768, out_features=2304, bias=True)\n                (proj): Linear4bit(in_features=768, out_features=768, bias=True)\n              )\n              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (mlp): MLPBlock(\n                (lin1): Linear4bit(in_features=768, out_features=3072, bias=True)\n                (lin2): Linear4bit(in_features=3072, out_features=768, bias=True)\n                (act): GELU(approximate='none')\n              )\n            )\n          )\n          (neck): Sequential(\n            (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): LayerNorm2d()\n            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (3): LayerNorm2d()\n          )\n          (net_2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (net_3): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        )\n        (vision_model): VitModel(\n          (embeddings): CLIPVisionEmbeddings(\n            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n            (position_embedding): Embedding(257, 1024)\n          )\n          (transformer): NoTPTransformer(\n            (layers): ModuleList(\n              (0-23): 24 x NoTPTransformerBlock(\n                (self_attn): NoTPAttention(\n                  (qkv_proj): Linear4bit(in_features=1024, out_features=3072, bias=True)\n                  (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n                )\n                (mlp): NoTPFeedForward(\n                  (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n                  (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n                )\n                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              )\n            )\n          )\n          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n        (projector): MlpProjector(\n          (layers): Linear4bit(in_features=2048, out_features=1280, bias=True)\n        )\n      )\n      (lm_head): Linear(in_features=1280, out_features=129280, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"from rapidfuzz.distance import Levenshtein\nimport random\n\n# Hàm chuẩn hóa (Quan trọng nhất)\ndef normalize_text(text):\n    if not isinstance(text, str): return \"\"\n    # Chuyển về chữ thường, xóa khoảng trắng thừa, xóa dấu chấm câu thừa\n    return text.lower().strip().strip('.')\n\ndef char_stats(gt: str, pr: str):\n    # Levenshtein distance = Số thao tác sửa đổi để biến pr thành gt\n    dist = Levenshtein.distance(gt, pr)\n    n = max(len(gt), 1)\n    cer_val = dist / n\n    \n    # Similarity: Giới hạn 0..1 (Nếu sai quá nhiều thì về 0)\n    sim = max(0.0, 1.0 - cer_val) \n    return dist, cer_val, sim\n\ndef eval_loop_plus(model, tokenizer, samples, prompt,\n                   base_size=1024, image_size=640, crop_mode=True, max_eval=None):\n    \n    # 1. Lấy mẫu ngẫu nhiên nếu dataset quá lớn\n    if max_eval is not None and len(samples) > max_eval:\n        samples = random.sample(samples, max_eval)\n\n    y_true, y_pred = [], []\n    exact = 0\n    total_chars = 0\n    total_dist = 0\n    total_sim = 0.0\n    debug_count = 0  # Đếm số lần in lỗi\n\n    print(f\"Đang đánh giá trên {len(samples)} mẫu...\")\n\n    for s in samples:\n        # --- BƯỚC 1: CHUẨN HÓA CẢ HAI ---\n        gt = normalize_text(s[\"text\"])\n        \n        # Gọi hàm dự đoán (giả định hàm ocr_one trả về string)\n        raw_pr = ocr_one(model, tokenizer, s[\"image_path\"], prompt, base_size, image_size, crop_mode)\n        pr = normalize_text(raw_pr) # <--- QUAN TRỌNG: Phải chuẩn hóa cả output của model\n\n        y_true.append(gt)\n        y_pred.append(pr)\n\n        # --- BƯỚC 2: SO SÁNH ---\n        if pr == gt:\n            exact += 1\n        else:\n            # DEBUG: In ra 5 mẫu sai đầu tiên để kiểm tra\n            if debug_count < 5:\n                print(f\"Sai: GT='{gt}' | PR='{pr}' (Raw: '{raw_pr}')\")\n                debug_count += 1\n\n        # --- BƯỚC 3: TÍNH TOÁN CHI TIẾT ---\n        dist, cer_i, sim_i = char_stats(gt, pr)\n        total_chars += max(len(gt), 1)\n        total_dist += dist\n        total_sim += sim_i\n\n    # Tính toán các chỉ số trung bình\n    n_samples = len(samples)\n    \n    # Accuracy (Chính xác tuyệt đối)\n    accuracy = exact / n_samples\n    \n    # Character Accuracy (Độ chính xác theo từng ký tự)\n    char_acc = 1.0 - (total_dist / total_chars) if total_chars > 0 else 0\n\n    return {\n        \"Accuracy (Exact Match)\": accuracy,       # Đây là cái bạn cần nhất (0.0 -> 1.0)\n        \"Char Accuracy (Soft)\": char_acc,         # Điểm an ủi (sai 1 chữ cái vẫn có điểm)\n        \"Avg Similarity\": total_sim / n_samples,\n        \"Total Samples\": n_samples,\n        \"Total Exact\": exact\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T05:12:44.004751Z","iopub.execute_input":"2025-12-14T05:12:44.005594Z","iopub.status.idle":"2025-12-14T05:12:44.015292Z","shell.execute_reply.started":"2025-12-14T05:12:44.005544Z","shell.execute_reply":"2025-12-14T05:12:44.014429Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"_, val_pairs = load_split(\"/kaggle/working/split.json\")\n\nPROMPT = \"<image>\\nHãy chép lại nguyên văn chữ viết tay trong ảnh. Giữ đúng dấu tiếng Việt. \"\n\n# Gọi hàm mới\nacc = eval_loop_plus(model, tokenizer, val_pairs, PROMPT, \n                     max_eval=200, \n                     base_size=1024, image_size=640, crop_mode=True)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T05:12:50.250755Z","iopub.execute_input":"2025-12-14T05:12:50.251372Z","iopub.status.idle":"2025-12-14T05:21:03.638309Z","shell.execute_reply.started":"2025-12-14T05:12:50.251349Z","shell.execute_reply":"2025-12-14T05:21:03.636451Z"}},"outputs":[{"name":"stdout","text":"Loaded split: /kaggle/working/split.json\nTrain: 99672 Val: 11074\nExample: {'image_path': '/kaggle/working/data/InkData_word_processed/20150305_0036_9415_2_tg_4_1_1.png', 'text': 'vô'}\nĐang đánh giá trên 200 mẫu...\n","output_type":"stream"},{"name":"stderr","text":"The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"công \nSai: GT='công' | PR='none' (Raw: 'none')\nchốn \nSai: GT='chốn' | PR='none' (Raw: 'none')\nviệc \nSai: GT='việc' | PR='none' (Raw: 'none')\ntra \nSai: GT='tra' | PR='none' (Raw: 'none')\nbị \nSai: GT='bị' | PR='none' (Raw: 'none')\nmất \ncó \ncầu \nđứng \nnghiên \nmẹ \ntôi \nmưa \nđã \nnhiều \nnơi \nxưa \nngười \nđoạn \nbiển \ncho \nPhi \ncho \nnhư \nvì \ngấm \n25 \nkhổ \nchưởng \nđi \ndân \ncon \n3 \nsử \nkhông \nngữ \nxin \nđúng \ncủa \ntrên \ngà \nngập \ncầu \ngiảm \nbốn \ncới \nvụt \nAn \nđất \nán \nty \nxe \ntư \nchỉ \nđơn \nđất \ngần \nlà \ntế \ntục \nchịu \nsức \nbắt \nmình \nhàng \nnăm \nthất \nđó \nthiện \nnước \nlập \nlẽ \nđánh \nviệc \nđiểm \nlực \nnày \nmâm \ngiảm \nnão \nmua \nhàng \ntiành \nsẽ \nbực \nNgo \nYến \ncấp \ntrưởng \nđược \nkhông \ntình \nđến \nkhám \ntrạm \nAn \ntình \nbể \nbiết \ngiấy \ngì \nTheo \ntế \nsau \nthị \ntế \nán \ncạnh \nhiện \ntạo \nrất \nnữ \nxe \nphút \nphát \nVN \nnhiều \ntiết \nsẽ \nthập \nlên \nthì thì \ntrưởng \nchỉ \nvệ \ntật \nđã \ndau \nBí \ngần \nmới \nđược \nchân \nquyền \ngà \ncác \nmua \n6 \ngà \nphiền \nbiết \nHau \nkhi \nSơn \nnhư \n5 \nLộ \nđến \ncủa \nnên \nsĩ \nbác \nđể \ncấp \nTài \ntrực \nđông \nngày \n23 \nvà \nnên \nđất \nliên \n3 \nnhư \nkhông \ntục \nphẩy \ný \nNgầm \ncơ \nlao \ncho \nnhóm \nbị \nSức \nnảy \nvăn \ncó \nmọi \ntầng \ntrang \ntâm \nkhoảng \ncác \nGiào \nbản \nthành \nbã \ntỏa \những \nhai \nsáng \nnhư \nlớm \nĐến \nnên \nbố \nChỉ \ncho \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2977597265.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                      base_size=1024, image_size=640, crop_mode=True)\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Final Accuracy: {acc:.2%}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to dict.__format__"],"ename":"TypeError","evalue":"unsupported format string passed to dict.__format__","output_type":"error"}],"execution_count":26},{"cell_type":"code","source":"print(f\"Final Accuracy: {acc}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T05:21:44.175286Z","iopub.execute_input":"2025-12-14T05:21:44.175533Z","iopub.status.idle":"2025-12-14T05:21:44.180416Z","shell.execute_reply.started":"2025-12-14T05:21:44.175517Z","shell.execute_reply":"2025-12-14T05:21:44.179612Z"}},"outputs":[{"name":"stdout","text":"Final Accuracy: {'Accuracy (Exact Match)': 0.0, 'Char Accuracy (Soft)': -0.1959247648902822, 'Avg Similarity': 0.042666666666666665, 'Total Samples': 200, 'Total Exact': 0}\n","output_type":"stream"}],"execution_count":28}]}