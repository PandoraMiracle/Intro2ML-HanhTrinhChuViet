{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"54ca4f86ebaf4f30b24876fd14217d0a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_169ef11619f541f4a50f08ab6dbb4d75","IPY_MODEL_48547cc4f5ea48b88533bf2e6a6dd8b8","IPY_MODEL_4c6a40dcad1645e6a2c0b8643576ba1e"],"layout":"IPY_MODEL_8d17d6845e6b494aaa06fa108986a0f4"}},"169ef11619f541f4a50f08ab6dbb4d75":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2bcc1ffe17ce4cefb9f8c21ec4c43491","placeholder":"‚Äã","style":"IPY_MODEL_eabb0ce1bc5f4401bc062cda7171ae7b","value":"Fetching‚Äá21‚Äáfiles:‚Äá100%"}},"48547cc4f5ea48b88533bf2e6a6dd8b8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e20992077ed49b880d1d52461fe5dfa","max":21,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4b748acb93b74f64967de117697d638a","value":21}},"4c6a40dcad1645e6a2c0b8643576ba1e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d154345b1b964cb293602e5223c6b4aa","placeholder":"‚Äã","style":"IPY_MODEL_2ca9991e95a949b3903be991090cd07c","value":"‚Äá21/21‚Äá[00:00&lt;00:00,‚Äá517.87it/s]"}},"8d17d6845e6b494aaa06fa108986a0f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2bcc1ffe17ce4cefb9f8c21ec4c43491":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eabb0ce1bc5f4401bc062cda7171ae7b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e20992077ed49b880d1d52461fe5dfa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b748acb93b74f64967de117697d638a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d154345b1b964cb293602e5223c6b4aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ca9991e95a949b3903be991090cd07c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dowload data","metadata":{"id":"dnTZpUl-3fnt"}},{"cell_type":"code","source":"!gdown 1BVjrYwkQ69SPXhABMpaWCwVie2OPlXL6\n!unzip -q data_word -d data\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cVayaK8srFdw","outputId":"c956c77b-7255-456b-aaf7-a3aee013086e","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:18:46.235011Z","iopub.execute_input":"2025-12-13T12:18:46.235183Z","iopub.status.idle":"2025-12-13T12:19:37.007179Z","shell.execute_reply.started":"2025-12-13T12:18:46.235167Z","shell.execute_reply":"2025-12-13T12:19:37.006165Z"}},"outputs":[{"name":"stdout","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=1BVjrYwkQ69SPXhABMpaWCwVie2OPlXL6\nFrom (redirected): https://drive.google.com/uc?id=1BVjrYwkQ69SPXhABMpaWCwVie2OPlXL6&confirm=t&uuid=6c001899-e77d-4b65-bf14-ad91d10684e0\nTo: /kaggle/working/data_word.zip\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.46G/1.46G [00:15<00:00, 93.7MB/s]\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Installation","metadata":{"id":"usawnG4l3ilN"}},{"cell_type":"code","source":"%%capture\nimport os, re\nif \"COLAB_\" not in \"\".join(os.environ.keys()):\n    !pip install unsloth\nelse:\n    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n    !pip install --no-deps unsloth\n!pip install transformers==4.56.2\n!pip install --no-deps trl==0.22.2\n!pip install jiwer\n!pip install einops addict easydict\n","metadata":{"id":"Jbx2HeqvwOAe","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:19:37.009354Z","iopub.execute_input":"2025-12-13T12:19:37.009585Z","iopub.status.idle":"2025-12-13T12:20:29.827351Z","shell.execute_reply.started":"2025-12-13T12:19:37.009562Z","shell.execute_reply":"2025-12-13T12:20:29.826449Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Unsloth","metadata":{"id":"yI58SfnL3nqd"}},{"cell_type":"code","source":"#!pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo transformers timm\n\nfrom huggingface_hub import snapshot_download\nsnapshot_download(\"unsloth/DeepSeek-OCR\", local_dir = \"deepseek_ocr\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":834,"referenced_widgets":["54ca4f86ebaf4f30b24876fd14217d0a","169ef11619f541f4a50f08ab6dbb4d75","48547cc4f5ea48b88533bf2e6a6dd8b8","4c6a40dcad1645e6a2c0b8643576ba1e","8d17d6845e6b494aaa06fa108986a0f4","2bcc1ffe17ce4cefb9f8c21ec4c43491","eabb0ce1bc5f4401bc062cda7171ae7b","0e20992077ed49b880d1d52461fe5dfa","4b748acb93b74f64967de117697d638a","d154345b1b964cb293602e5223c6b4aa","2ca9991e95a949b3903be991090cd07c"]},"id":"KwbsPtwIxZWP","outputId":"064c39dd-deea-4dc8-a595-11985c1cdc32","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:20:29.828448Z","iopub.execute_input":"2025-12-13T12:20:29.829063Z","iopub.status.idle":"2025-12-13T12:20:48.573601Z","shell.execute_reply.started":"2025-12-13T12:20:29.829036Z","shell.execute_reply":"2025-12-13T12:20:48.572829Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Fetching 21 files:   0%|          | 0/21 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c4880f8d6cd4162abdc302ef8fe4e57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"LICENSE: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b19ab25806b4f1bae80919ad08f1b9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ceb2a08295ce4f5e8f51c4bda12f2e50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90a8248d4bd945fcb161cdca24b662e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"assets/show2.jpg:   0%|          | 0.00/216k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b73f8c446f9043bfa08936c8290ae72e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"assets/show3.jpg:   0%|          | 0.00/247k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dae68605c0e4537b0be64eafa02a1f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"assets/fig1.png:   0%|          | 0.00/396k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49399ff45c11427b968d6088a1cab0d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README-checkpoint.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46ccdb10a3784f3688dc7018d226c7cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"assets/show1.jpg:   0%|          | 0.00/117k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a318e16bf56e410fbfd99e59ad0da880"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"728126e8c4cf4467ad07b0b05f6052cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_deepseek_v2.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3121afc5c34f4987a1ed873649e623f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"assets/show4.jpg:   0%|          | 0.00/269k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"320081d90ecc44e6b40b790bf50ddf2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"conversation.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13dbe9e4ad8146ec9bed1d5bbc01f46e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"deepencoder.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f6ecb035a854ed1adffeb7fbbd638a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-000001.safetensors:   0%|          | 0.00/6.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04e7568cea944a7496750a3822ba886d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"772a0d6d2e2641469dd39c8ab35d6bd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modeling_deepseekocr.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cbe518391274d6e83ad5a083b24f41c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modeling_deepseekv2.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2cc5ec1a37541b3aaecadfe6bdc0168"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"processor_config.json:   0%|          | 0.00/460 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f22f40c4d89242eb886671f4c02ec346"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/801 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7091f4a6fe9a44de949cfd14dadc4137"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"710af4663e4f4f6ebeed970653a48bc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64fb0b235964426ca0b0b66401f1978d"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/deepseek_ocr'"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import os, logging\nos.environ[\"UNSLOTH_WARN_UNINITIALIZED\"] = \"0\"\n\n# G·ª° c√°c handler d·∫°ng Unsloth kh·ªèi logger transformers.modeling_utils\nlogger = logging.getLogger(\"transformers.modeling_utils\")\nnew_handlers = []\nfor h in logger.handlers:\n    # Unsloth th∆∞·ªùng ƒë·∫∑t handler c√≥ class name ƒë·∫∑c tr∆∞ng, ta l·ªçc theo t√™n\n    if \"Unsloth\" in h.__class__.__name__:\n        continue\n    new_handlers.append(h)\nlogger.handlers = new_handlers\n","metadata":{"id":"G5DV_9_h3G9k","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:20:48.574564Z","iopub.execute_input":"2025-12-13T12:20:48.574858Z","iopub.status.idle":"2025-12-13T12:20:48.581017Z","shell.execute_reply.started":"2025-12-13T12:20:48.574838Z","shell.execute_reply":"2025-12-13T12:20:48.579771Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Import","metadata":{"id":"8EHBWnuSV7d9"}},{"cell_type":"code","source":"import os, json, random, argparse, unicodedata\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Any, Tuple\n\nimport torch\nfrom jiwer import wer, cer\nfrom huggingface_hub import snapshot_download\n\nfrom unsloth import FastVisionModel, is_bf16_supported\nfrom transformers import AutoModel, Trainer, TrainingArguments","metadata":{"id":"b5VLXfUWV84G","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:20:48.581813Z","iopub.execute_input":"2025-12-13T12:20:48.582217Z","iopub.status.idle":"2025-12-13T12:22:07.809734Z","shell.execute_reply.started":"2025-12-13T12:20:48.582193Z","shell.execute_reply":"2025-12-13T12:22:07.808829Z"}},"outputs":[{"name":"stdout","text":"ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-12-13 12:21:18.243077: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765628478.629207      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765628478.776146      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"ü¶• Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Fine-Tuning","metadata":{"id":"ZpwFkK_BWcqy"}},{"cell_type":"code","source":"# ========= (1) Text utils =========\ndef normalize_text(s: str) -> str:\n    # Gi·ªØ d·∫•u ti·∫øng Vi·ªát ·ªïn ƒë·ªãnh (NFC), normalize whitespace nh·∫π\n    s = unicodedata.normalize(\"NFC\", s or \"\")\n    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n    s = \" \".join(s.split())\n    return s.strip()\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)","metadata":{"id":"tcYFagGoWhVu","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:22:07.810564Z","iopub.execute_input":"2025-12-13T12:22:07.810827Z","iopub.status.idle":"2025-12-13T12:22:07.815530Z","shell.execute_reply.started":"2025-12-13T12:22:07.810803Z","shell.execute_reply":"2025-12-13T12:22:07.814998Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Data Prep","metadata":{"id":"aKE43nWjWtyN"}},{"cell_type":"code","source":"# ========= (2) Data loader: scan .png/.txt pairs =========\ndef collect_pairs(root: str, recursive: bool = True) -> List[Dict[str, str]]:\n    \"\"\"\n    Expect:\n      <stem>.png\n      <stem>.txt   (ground truth)\n    Return list of {\"image_path\": ..., \"text\": ...}\n    \"\"\"\n    items = []\n    walker = os.walk(root) if recursive else [(root, [], os.listdir(root))]\n    for d, _, files in walker:\n        pngs = [f for f in files if f.lower().endswith(\".png\")]\n        for png in pngs:\n            stem = png[:-4]\n            txt = stem + \".txt\"\n            img_path = os.path.join(d, png)\n            txt_path = os.path.join(d, txt)\n            if not os.path.exists(txt_path):\n                continue\n            with open(txt_path, \"r\", encoding=\"utf-8-sig\") as f:\n                gt = normalize_text(f.read())\n            if gt == \"\":\n                continue\n            items.append({\"image_path\": img_path, \"text\": gt})\n    return items\n\n# ========= (3) Conversation format =========\ndef to_conversation(sample: Dict[str, str], instruction: str) -> Dict[str, Any]:\n    # NOTE: ƒë·ªÉ nh·∫π RAM, ta truy·ªÅn image_path (string) v√† collator s·∫Ω t·ª± Image.open\n    return {\n        \"messages\": [\n            {\"role\": \"<|User|>\", \"content\": instruction, \"images\": [sample[\"image_path\"]]},\n            {\"role\": \"<|Assistant|>\", \"content\": sample[\"text\"]},\n        ]\n    }\n","metadata":{"id":"H9GcPNPRW6hM","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:22:07.817526Z","iopub.execute_input":"2025-12-13T12:22:07.817921Z","iopub.status.idle":"2025-12-13T12:22:07.839408Z","shell.execute_reply.started":"2025-12-13T12:22:07.817903Z","shell.execute_reply":"2025-12-13T12:22:07.838909Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Create datacollator","metadata":{"id":"IwWQaX4CW9kQ"}},{"cell_type":"code","source":"# ========= (4) Collator (d·ª±a tr√™n class b·∫°n ƒëang d√πng, th√™m h·ªó tr·ª£ path string) =========\nimport math, io\nfrom PIL import Image, ImageOps\nfrom torch.nn.utils.rnn import pad_sequence\nfrom deepseek_ocr.modeling_deepseekocr import text_encode, BasicImageTransform, dynamic_preprocess\n\nclass DeepSeekOCRDataCollator:\n    def __init__(self, tokenizer, model, image_size=640, base_size=1024, crop_mode=True, train_on_responses_only=True):\n        self.tokenizer = tokenizer\n        self.model = model\n        self.image_size = image_size\n        self.base_size = base_size\n        self.crop_mode = crop_mode\n        self.image_token_id = 128815\n        self.dtype = model.dtype\n        self.train_on_responses_only = train_on_responses_only\n\n        self.image_transform = BasicImageTransform(\n            mean=(0.5, 0.5, 0.5),\n            std=(0.5, 0.5, 0.5),\n            normalize=True\n        )\n        self.patch_size = 16\n        self.downsample_ratio = 4\n\n        self.bos_id = tokenizer.bos_token_id if getattr(tokenizer, \"bos_token_id\", None) is not None else 0\n\n    def deserialize_image(self, image_data) -> Image.Image:\n        # ====== PATCH quan tr·ªçng cho dataset c·ªßa b·∫°n: support path string ======\n        if isinstance(image_data, str):\n            return Image.open(image_data).convert(\"RGB\")\n        if isinstance(image_data, Image.Image):\n            return image_data.convert(\"RGB\")\n        if isinstance(image_data, dict):\n            if \"bytes\" in image_data:\n                image = Image.open(io.BytesIO(image_data[\"bytes\"]))\n                return image.convert(\"RGB\")\n            if \"path\" in image_data:\n                return Image.open(image_data[\"path\"]).convert(\"RGB\")\n        raise ValueError(f\"Unsupported image format: {type(image_data)}\")\n\n    def process_image(self, image: Image.Image):\n        images_list, images_crop_list, images_spatial_crop = [], [], []\n\n        if self.crop_mode:\n            if image.size[0] <= 640 and image.size[1] <= 640:\n                crop_ratio = (1, 1)\n                images_crop_raw = []\n            else:\n                images_crop_raw, crop_ratio = dynamic_preprocess(\n                    image, min_num=2, max_num=9, image_size=self.image_size, use_thumbnail=False\n                )\n\n            global_view = ImageOps.pad(\n                image, (self.base_size, self.base_size),\n                color=tuple(int(x * 255) for x in self.image_transform.mean)\n            )\n            images_list.append(self.image_transform(global_view).to(self.dtype))\n\n            width_crop_num, height_crop_num = crop_ratio\n            images_spatial_crop.append([width_crop_num, height_crop_num])\n\n            if width_crop_num > 1 or height_crop_num > 1:\n                for crop_img in images_crop_raw:\n                    images_crop_list.append(self.image_transform(crop_img).to(self.dtype))\n\n            num_queries = math.ceil((self.image_size // self.patch_size) / self.downsample_ratio)\n            num_queries_base = math.ceil((self.base_size // self.patch_size) / self.downsample_ratio)\n\n            tokenized_image = ([self.image_token_id] * num_queries_base + [self.image_token_id]) * num_queries_base\n            tokenized_image += [self.image_token_id]\n            if width_crop_num > 1 or height_crop_num > 1:\n                tokenized_image += ([self.image_token_id] * (num_queries * width_crop_num) + [self.image_token_id]) * (\n                    num_queries * height_crop_num\n                )\n        else:\n            crop_ratio = (1, 1)\n            images_spatial_crop.append([1, 1])\n\n            global_view = ImageOps.pad(\n                image, (self.base_size, self.base_size),\n                color=tuple(int(x * 255) for x in self.image_transform.mean)\n            )\n            images_list.append(self.image_transform(global_view).to(self.dtype))\n\n            num_queries = math.ceil((self.base_size // self.patch_size) / self.downsample_ratio)\n            tokenized_image = ([self.image_token_id] * num_queries + [self.image_token_id]) * num_queries\n            tokenized_image += [self.image_token_id]\n\n        return images_list, images_crop_list, images_spatial_crop, tokenized_image\n\n    def process_single_sample(self, messages: List[Dict]) -> Dict[str, Any]:\n        images = []\n        for m in messages:\n            for img in m.get(\"images\", []) or []:\n                images.append(self.deserialize_image(img))\n\n        tokenized_str, images_seq_mask = [], []\n        images_list, images_crop_list, images_spatial_crop = [], [], []\n        prompt_token_count = -1\n        assistant_started = False\n        image_idx = 0\n\n        tokenized_str.append(self.bos_id)\n        images_seq_mask.append(False)\n\n        for m in messages:\n            role = m[\"role\"]\n            content = m[\"content\"]\n\n            if role == \"<|Assistant|>\":\n                if not assistant_started:\n                    prompt_token_count = len(tokenized_str)\n                    assistant_started = True\n                content = f\"{content.strip()} {self.tokenizer.eos_token}\"\n\n            text_splits = content.split(\"<image>\")\n            for i, text_part in enumerate(text_splits):\n                tok = text_encode(self.tokenizer, text_part, bos=False, eos=False)\n                tokenized_str.extend(tok)\n                images_seq_mask.extend([False] * len(tok))\n\n                if i < len(text_splits) - 1:\n                    img = images[image_idx]\n                    il, cl, sc, tok_img = self.process_image(img)\n                    images_list.extend(il)\n                    images_crop_list.extend(cl)\n                    images_spatial_crop.extend(sc)\n\n                    tokenized_str.extend(tok_img)\n                    images_seq_mask.extend([True] * len(tok_img))\n                    image_idx += 1\n\n        if not assistant_started:\n            prompt_token_count = len(tokenized_str)\n\n        images_ori = torch.stack(images_list, dim=0)\n        images_spatial_crop_tensor = torch.tensor(images_spatial_crop, dtype=torch.long)\n        images_crop = torch.stack(images_crop_list, dim=0) if images_crop_list else torch.zeros(\n            (1, 3, self.base_size, self.base_size), dtype=self.dtype\n        )\n\n        return {\n            \"input_ids\": torch.tensor(tokenized_str, dtype=torch.long),\n            \"images_seq_mask\": torch.tensor(images_seq_mask, dtype=torch.bool),\n            \"images_ori\": images_ori,\n            \"images_crop\": images_crop,\n            \"images_spatial_crop\": images_spatial_crop_tensor,\n            \"prompt_token_count\": prompt_token_count,\n        }\n\n    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n        batch_data = [self.process_single_sample(f[\"messages\"]) for f in features]\n\n        input_ids_list = [x[\"input_ids\"] for x in batch_data]\n        images_seq_mask_list = [x[\"images_seq_mask\"] for x in batch_data]\n        prompt_counts = [x[\"prompt_token_count\"] for x in batch_data]\n\n        input_ids = pad_sequence(input_ids_list, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n        images_seq_mask = pad_sequence(images_seq_mask_list, batch_first=True, padding_value=False)\n\n        labels = input_ids.clone()\n        labels[labels == self.tokenizer.pad_token_id] = -100\n        labels[images_seq_mask] = -100\n        for i, pc in enumerate(prompt_counts):\n            if pc > 0:\n                labels[i, :pc] = -100\n\n        attention_mask = (input_ids != self.tokenizer.pad_token_id).long()\n        images_batch = [(x[\"images_crop\"], x[\"images_ori\"]) for x in batch_data]\n        images_spatial_crop = torch.cat([x[\"images_spatial_crop\"] for x in batch_data], dim=0)\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels,\n            \"images\": images_batch,\n            \"images_seq_mask\": images_seq_mask,\n            \"images_spatial_crop\": images_spatial_crop,\n        }\n","metadata":{"id":"PLezklY0Xi6O","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:22:07.839988Z","iopub.execute_input":"2025-12-13T12:22:07.840191Z","iopub.status.idle":"2025-12-13T12:22:07.942646Z","shell.execute_reply.started":"2025-12-13T12:22:07.840173Z","shell.execute_reply":"2025-12-13T12:22:07.942026Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Inference and eval","metadata":{"id":"zWgEidgJXl9I"}},{"cell_type":"code","source":"# ========= (5) Inference helper =========\n@torch.inference_mode()\ndef ocr_one(model, tokenizer, image_path: str, prompt: str, base_size=1024, image_size=640, crop_mode=True) -> str:\n    out = model.infer(\n        tokenizer,\n        prompt=prompt,\n        image_file=image_path,\n        output_path=\"outputs\",\n        base_size=base_size,\n        image_size=image_size,\n        crop_mode=crop_mode,\n        save_results=False,\n        test_compress=False,\n    )\n    # robust parse\n    if isinstance(out, str):\n        return normalize_text(out)\n    if isinstance(out, list) and len(out) > 0:\n        return normalize_text(str(out[0]))\n    if isinstance(out, dict):\n        for k in [\"text\", \"result\", \"output\"]:\n            if k in out:\n                return normalize_text(str(out[k]))\n    return normalize_text(str(out))\n\ndef eval_loop(model, tokenizer, samples: List[Dict[str, str]], prompt: str,\n              base_size=1024, image_size=640, crop_mode=True, max_eval=None) -> Dict[str, float]:\n    if max_eval is not None:\n        samples = samples[:max_eval]\n\n    y_true, y_pred = [], []\n    for s in samples:\n        gt = normalize_text(s[\"text\"])\n        pr = ocr_one(model, tokenizer, s[\"image_path\"], prompt, base_size, image_size, crop_mode)\n        y_true.append(gt)\n        y_pred.append(pr)\n\n    return {\n        \"cer\": float(cer(y_true, y_pred)),\n        \"wer\": float(wer(y_true, y_pred)),\n        \"n\": len(samples),\n    }","metadata":{"id":"SSM_jT7mXvKz","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:22:07.943420Z","iopub.execute_input":"2025-12-13T12:22:07.943668Z","iopub.status.idle":"2025-12-13T12:22:07.951421Z","shell.execute_reply.started":"2025-12-13T12:22:07.943650Z","shell.execute_reply":"2025-12-13T12:22:07.950595Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Main","metadata":{"id":"3sOW09JWXwup"}},{"cell_type":"code","source":"# ========= (6) Main =========\ndef main1():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--data_dir\", type=str, required=True)\n    ap.add_argument(\"--out_dir\", type=str, default=\"outputs/handwriting_lora\")\n    ap.add_argument(\"--val_ratio\", type=float, default=0.05)\n    ap.add_argument(\"--seed\", type=int, default=3407)\n\n    ap.add_argument(\"--max_steps\", type=int, default=800)\n    ap.add_argument(\"--lr\", type=float, default=2e-4)\n    ap.add_argument(\"--bsz\", type=int, default=2)\n    ap.add_argument(\"--gas\", type=int, default=4)\n\n    ap.add_argument(\"--image_size\", type=int, default=640)\n    ap.add_argument(\"--base_size\", type=int, default=1024)\n    ap.add_argument(\"--crop_mode\", action=\"store_true\")\n\n    ap.add_argument(\"--max_eval\", type=int, default=200)  # eval nhanh\n    args = ap.parse_args()\n\n    set_seed(args.seed)\n    os.makedirs(args.out_dir, exist_ok=True)\n\n    # Prompt: n√™n ‚Äúc·ª©ng‚Äù v√† nh·∫•t qu√°n cho ch·ªØ vi·∫øt tay\n    PROMPT = \"<image>\\nH√£y ch√©p l·∫°i nguy√™n vƒÉn ch·ªØ vi·∫øt tay trong ·∫£nh. Gi·ªØ ƒë√∫ng d·∫•u ti·∫øng Vi·ªát. \"\n\n    # 1) Download model snapshot\n    #snapshot_download(\"unsloth/DeepSeek-OCR\", local_dir=\"deepseek_ocr\")\n\n    # 2) Load base model\n    model, tokenizer = FastVisionModel.from_pretrained(\n        \"unsloth/DeepSeek-OCR\",\n        load_in_4bit=False,\n        auto_model=AutoModel,\n        trust_remote_code=True,\n        unsloth_force_compile=True,\n        use_gradient_checkpointing=\"unsloth\",\n    )\n\n\n    # 3) Load dataset\n    import json, random\n\n    pairs = collect_pairs(args.data_dir, recursive=True)\n    rng = random.Random(42)\n    rng.shuffle(pairs)\n\n    n_val = max(1, int(len(pairs)*0.1))\n    split = {\"train\": pairs[n_val:], \"val\": pairs[:n_val]}\n\n    # Save train/val\n    with open(\"split.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(split, f, ensure_ascii=False, indent=2)\n    train_pairs = split[\"train\"]\n    val_pairs = split[\"val\"]\n    print(f\"Total: {len(pairs)} | Train: {len(train_pairs)} | Val: {len(val_pairs)}\")\n\n    # 4) Baseline eval before finetune\n    FastVisionModel.for_inference(model)\n    base_metrics = eval_loop(\n        model, tokenizer, val_pairs, PROMPT,\n        base_size=args.base_size, image_size=args.image_size, crop_mode=args.crop_mode,\n        max_eval=args.max_eval\n    )\n    print(\"Baseline:\", base_metrics)\n\n    # 5) Add LoRA (same target_modules)\n    FastVisionModel.for_training(model)\n    model = FastVisionModel.get_peft_model(\n        model,\n        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n        r=16,\n        lora_alpha=16,\n        lora_dropout=0,\n        bias=\"none\",\n        random_state=args.seed,\n        use_rslora=False,\n        loftq_config=None,\n    )\n\n    # 6) Convert to conversations\n    train_conv = [to_conversation(s, PROMPT) for s in train_pairs]\n    val_conv = [to_conversation(s, PROMPT) for s in val_pairs]\n\n    # 7) Trainer\n    collator = DeepSeekOCRDataCollator(\n        tokenizer=tokenizer,\n        model=model,\n        image_size=args.image_size,\n        base_size=args.base_size,\n        crop_mode=args.crop_mode,\n        train_on_responses_only=True,\n    )\n\n    targs = TrainingArguments(\n        output_dir=args.out_dir,\n        per_device_train_batch_size=args.bsz,\n        gradient_accumulation_steps=args.gas,\n        learning_rate=args.lr,\n        warmup_steps=20,\n        max_steps=args.max_steps,\n        logging_steps=10,\n        save_steps=200,\n        optim=\"adamw_8bit\",\n        weight_decay=0.001,\n        lr_scheduler_type=\"linear\",\n        seed=args.seed,\n        fp16=not is_bf16_supported(),\n        bf16=is_bf16_supported(),\n        report_to=\"none\",\n        dataloader_num_workers=2,\n        remove_unused_columns=False,  # quan tr·ªçng cho vision finetune\n    )\n\n    trainer = Trainer(\n        model=model,\n        tokenizer=tokenizer,\n        data_collator=collator,\n        train_dataset=train_conv,\n        args=targs,\n    )\n\n    trainer.train()\n\n    # 8) Save LoRA adapters\n    model.save_pretrained(os.path.join(args.out_dir, \"lora_model\"))\n    tokenizer.save_pretrained(os.path.join(args.out_dir, \"lora_model\"))\n\n    # 9) Eval after finetune\n    FastVisionModel.for_inference(model)\n    ft_metrics = eval_loop(\n        model, tokenizer, val_pairs, PROMPT,\n        base_size=args.base_size, image_size=args.image_size, crop_mode=args.crop_mode,\n        max_eval=args.max_eval\n    )\n    print(\"Finetuned:\", ft_metrics)\n\n    # 10) Write report\n    report = {\"baseline\": base_metrics, \"finetuned\": ft_metrics, \"train_size\": len(train_pairs), \"val_size\": len(val_pairs)}\n    with open(os.path.join(args.out_dir, \"eval_report.json\"), \"w\", encoding=\"utf-8\") as f:\n        json.dump(report, f, ensure_ascii=False, indent=2)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Se4oz-jGq239","outputId":"ac1c4199-bd9e-41a6-f02c-a97207e78c24","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:22:07.952282Z","iopub.execute_input":"2025-12-13T12:22:07.953010Z","iopub.status.idle":"2025-12-13T12:22:07.982810Z","shell.execute_reply.started":"2025-12-13T12:22:07.952989Z","shell.execute_reply":"2025-12-13T12:22:07.982041Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Run","metadata":{"id":"WJraBeJRYdRW"}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n  import sys\n  DATA_DIR = \"/kaggle/working/data/InkData_word_processed\"\n  sys.argv = [\"train_handwriting.py\",\n              \"--data_dir\", DATA_DIR,\n              \"--out_dir\", \"outputs/handwriting_lora\",\n              \"--max_steps\", \"1500\",\n              \"--crop_mode\"]\n  main1()","metadata":{"id":"Z8TFswYWWWeU","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:22:07.983532Z","iopub.execute_input":"2025-12-13T12:22:07.983869Z"}},"outputs":[{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/unsloth/DeepSeek-OCR:\n- deepencoder.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/unsloth/DeepSeek-OCR:\n- conversation.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/unsloth/DeepSeek-OCR:\n- modeling_deepseekocr.py\n- deepencoder.py\n- conversation.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nYou are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: WARNING `trust_remote_code` is True.\nAre you certain you want to do remote code execution?\n==((====))==  Unsloth 2025.12.5: Fast Deepseekocr patching. Transformers: 4.56.2.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nUnsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n","output_type":"stream"},{"name":"stderr","text":"You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\nYou are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d3ad5a2bb134c0b94cec7ade86a466b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-000001.safetensors:   0%|          | 0.00/6.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5f23c603a614095b71a5d835e88e3e8"}},"metadata":{}},{"name":"stderr","text":"Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at unsloth/DeepSeek-OCR and are newly initialized: ['model.vision_model.embeddings.position_ids']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df911cfa0f524d6d912bce5342c66e77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27cc9689179f411d81a9cbd4d0837ef1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/801 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dde6f01f481e44b6b91297c3ba3fec7b"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# Inference test\n\nimport os, json, argparse, unicodedata\nimport torch\nfrom huggingface_hub import snapshot_download\nfrom unsloth import FastVisionModel\nfrom transformers import AutoModel\n\ndef normalize_text(s: str) -> str:\n    s = unicodedata.normalize(\"NFC\", s or \"\")\n    s = \" \".join(s.split())\n    return s.strip()\n\n@torch.inference_mode()\ndef main2():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--image_dir\", required=True)\n    ap.add_argument(\"--lora_dir\", required=True)\n    ap.add_argument(\"--out_json\", default=\"preds.json\")\n    ap.add_argument(\"--base_size\", type=int, default=1024)\n    ap.add_argument(\"--image_size\", type=int, default=640)\n    ap.add_argument(\"--crop_mode\", action=\"store_true\")\n    args = ap.parse_args()\n\n    PROMPT = \"<image>\\nH√£y ch√©p l·∫°i nguy√™n vƒÉn ch·ªØ vi·∫øt tay trong ·∫£nh. Gi·ªØ ƒë√∫ng d·∫•u ti·∫øng Vi·ªát. \"\n\n    snapshot_download(\"unsloth/DeepSeek-OCR\", local_dir=\"deepseek_ocr_base\")\n    model, tokenizer = FastVisionModel.from_pretrained(\n        args.lora_dir,\n        load_in_4bit=False,\n        auto_model=AutoModel,\n        trust_remote_code=True,\n        unsloth_force_compile=True,\n        use_gradient_checkpointing=\"unsloth\",\n    )\n    FastVisionModel.for_inference(model)\n\n    results = {}\n    for f in sorted(os.listdir(args.image_dir)):\n        if not f.lower().endswith(\".png\"):\n            continue\n        p = os.path.join(args.image_dir, f)\n        out = model.infer(tokenizer, prompt=PROMPT, image_file=p, output_path=\"outputs\",\n                          base_size=args.base_size, image_size=args.image_size, crop_mode=args.crop_mode,\n                          save_results=False, test_compress=False)\n        results[f] = normalize_text(out if isinstance(out, str) else str(out))\n\n    with open(args.out_json, \"w\", encoding=\"utf-8\") as fp:\n        json.dump(results, fp, ensure_ascii=False, indent=2)\n","metadata":{"id":"Qo1uhDiQAqdF","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n  import sys\n  INF_DIR = \"/kaggle/working/data/InkData_word_processed/20140603_0003_BCCTC_tg_0_0_0.png\"\n  sys.argv = [\"infer_handwriting.py\",\n              \"--image_dir\", INF_DIR,\n              \"--lora_dir\", \"lora_model\",\n              \"--base_size\",\n              \"--image_size\",\n              \"--crop_mode\"]\n  main2()\n","metadata":{"id":"aINo99_hYtSH","trusted":true},"outputs":[],"execution_count":null}]}