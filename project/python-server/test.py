# coding: utf-8
import torch
from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM

# Model Vietnamese khÃ´ng bá»‹ gated - cÃ³ thá»ƒ táº£i ngay
MODEL_ID = "vilm/vinallama-7b-chat"  

# ============================================================
# MÃ” HÃŒNH Sá»¬ Dá»¤NG
# ============================================================
print("=" * 60)
print("MÃ” HÃŒNH Sá»¬ Dá»¤NG: VinaLLaMA-7B-Chat")
print("=" * 60)
print("""
ğŸ“Œ Äá»‹nh nghÄ©a:
   VinaLLaMA-7B-Chat lÃ  má»™t Large Language Model (LLM) Ä‘Æ°á»£c fine-tune 
   tá»« LLaMA-2 7B cho tiáº¿ng Viá»‡t, tá»‘i Æ°u cho cÃ¡c tÃ¡c vá»¥ Ä‘á»‘i thoáº¡i (chat).

ğŸ“Œ Reference:
   - HuggingFace: https://huggingface.co/vilm/vinallama-7b-chat
   - Base model: Meta LLaMA-2 7B
   - Fine-tuned by: VILM Team

ğŸ“Œ LÃ½ do chá»n model:
   1. Pretrain/Fine-tune trÃªn dá»¯ liá»‡u tiáº¿ng Viá»‡t â†’ hiá»ƒu ngá»¯ cáº£nh Viá»‡t tá»‘t
   2. KÃ­ch thÆ°á»›c 7B params â†’ cÃ¢n báº±ng giá»¯a hiá»‡u suáº¥t vÃ  tÃ i nguyÃªn
   3. KhÃ´ng bá»‹ gated (public) â†’ dá»… dÃ ng táº£i vÃ  sá»­ dá»¥ng
   4. Dá»±a trÃªn LLaMA-2 â†’ kiáº¿n trÃºc hiá»‡n Ä‘áº¡i, hiá»‡u quáº£
   5. Há»— trá»£ chat format â†’ phÃ¹ há»£p cho á»©ng dá»¥ng chatbot
""")

# 1) Load config / tokenizer / model
print("\nâ³ Äang táº£i model...")
config = AutoConfig.from_pretrained(MODEL_ID, trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    trust_remote_code=True,
    device_map="auto",     # tá»± phÃ¢n bá»‘ lÃªn GPU/CPU náº¿u cÃ³
    torch_dtype="auto"
)
print("âœ… Táº£i model thÃ nh cÃ´ng!")

# ============================================================
# KIáº¾N TRÃšC CHI TIáº¾T
# ============================================================
print("\n" + "=" * 60)
print("KIáº¾N TRÃšC CHI TIáº¾T (LLaMA-2 Architecture)")
print("=" * 60)
print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VinaLLaMA-7B-Chat                        â”‚
â”‚                  (LLaMA-2 Architecture)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Input Text                                                 â”‚
â”‚      â†“                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Tokenizer (BPE - Byte Pair Encoding)               â”‚   â”‚
â”‚  â”‚  vocab_size = 32,000                                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚      â†“                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Embedding Layer                                     â”‚   â”‚
â”‚  â”‚  (32000 Ã— 4096)                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚      â†“                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         Transformer Decoder Blocks Ã— 32             â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  RMSNorm (Root Mean Square Normalization)     â”‚  â”‚   â”‚
â”‚  â”‚  â”‚      â†“                                        â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  Multi-Head Attention (GQA)                   â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ n_heads = 32                               â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ n_kv_heads = 32                            â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ head_dim = 128                             â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ RoPE (Rotary Position Embedding)           â”‚  â”‚   â”‚
â”‚  â”‚  â”‚      â†“                                        â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  Residual Connection                          â”‚  â”‚   â”‚
â”‚  â”‚  â”‚      â†“                                        â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  RMSNorm                                      â”‚  â”‚   â”‚
â”‚  â”‚  â”‚      â†“                                        â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  FFN (Feed Forward Network) - SwiGLU          â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ gate_proj: 4096 â†’ 11008                    â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ up_proj:   4096 â†’ 11008                    â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ down_proj: 11008 â†’ 4096                    â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Activation: SiLU (Swish)                   â”‚  â”‚   â”‚
â”‚  â”‚  â”‚      â†“                                        â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  Residual Connection                          â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚      â†“                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Final RMSNorm                                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚      â†“                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  LM Head (Linear: 4096 â†’ 32000)                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚      â†“                                                      â”‚
â”‚  Output Logits â†’ Softmax â†’ Next Token Prediction           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")

# ============================================================
# MÃ” Táº¢ THAM Sá» Cáº¤U HÃŒNH
# ============================================================
print("=" * 60)
print("THAM Sá» Cáº¤U HÃŒNH (CONFIG)")
print("=" * 60)
config_params = {
    "model_type": "Loáº¡i mÃ´ hÃ¬nh",
    "hidden_size": "KÃ­ch thÆ°á»›c hidden state (d_model)",
    "intermediate_size": "KÃ­ch thÆ°á»›c FFN intermediate",
    "num_hidden_layers": "Sá»‘ lá»›p Transformer",
    "num_attention_heads": "Sá»‘ attention heads",
    "num_key_value_heads": "Sá»‘ KV heads (GQA)",
    "vocab_size": "KÃ­ch thÆ°á»›c vocabulary",
    "max_position_embeddings": "Äá»™ dÃ i context tá»‘i Ä‘a",
    "rms_norm_eps": "Epsilon cho RMSNorm",
    "rope_theta": "Theta cho RoPE",
    "hidden_act": "HÃ m kÃ­ch hoáº¡t",
}

for key, desc in config_params.items():
    if hasattr(config, key):
        value = getattr(config, key)
        print(f"  {desc:35} â”‚ {key:25} = {value}")

# ============================================================
# Sá» LÆ¯á»¢NG THAM Sá»
# ============================================================
print("\n" + "=" * 60)
print("Sá» LÆ¯á»¢NG THAM Sá» (PARAMETERS)")
print("=" * 60)
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"  Total parameters:      {total_params:>15,}")
print(f"  Trainable parameters:  {trainable_params:>15,}")
print(f"  Non-trainable:         {total_params - trainable_params:>15,}")
print(f"  Model size (float16):  ~{total_params * 2 / 1e9:.2f} GB")
print(f"  Model size (float32):  ~{total_params * 4 / 1e9:.2f} GB")

# Chi tiáº¿t tá»«ng layer
print("\nğŸ“Š PhÃ¢n bá»‘ tham sá»‘ theo layer:")
param_breakdown = {}
for name, param in model.named_parameters():
    layer_type = name.split('.')[0] if '.' in name else name
    if layer_type not in param_breakdown:
        param_breakdown[layer_type] = 0
    param_breakdown[layer_type] += param.numel()

for layer, count in param_breakdown.items():
    percentage = count / total_params * 100
    print(f"  {layer:20} â”‚ {count:>15,} ({percentage:5.2f}%)")

# ============================================================
# HÃ€M KÃCH HOáº T
# ============================================================
print("\n" + "=" * 60)
print("HÃ€M KÃCH HOáº T (ACTIVATION FUNCTION)")
print("=" * 60)
print(f"""
ğŸ“Œ Activation Ä‘Æ°á»£c sá»­ dá»¥ng: {config.hidden_act.upper() if hasattr(config, 'hidden_act') else 'N/A'}

ğŸ“Œ Chi tiáº¿t vá» SiLU (Swish):
   â€¢ CÃ´ng thá»©c: SiLU(x) = x * Ïƒ(x) = x * (1 / (1 + e^(-x)))
   â€¢ LÃ  smooth approximation cá»§a ReLU
   â€¢ CÃ³ Ä‘áº¡o hÃ m mÆ°á»£t â†’ huáº¥n luyá»‡n á»•n Ä‘á»‹nh hÆ¡n

ğŸ“Œ SwiGLU (Swish-Gated Linear Unit):
   â€¢ ÄÆ°á»£c sá»­ dá»¥ng trong FFN layer cá»§a LLaMA
   â€¢ CÃ´ng thá»©c: SwiGLU(x) = SiLU(W_gate Â· x) âŠ™ (W_up Â· x)
   â€¢ Hiá»‡u quáº£ hÆ¡n GELU/ReLU cho LLM
   
ğŸ“Œ So sÃ¡nh vá»›i cÃ¡c activation khÃ¡c:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Activation â”‚ CÃ´ng thá»©c                       â”‚ Äáº·c Ä‘iá»ƒm    â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ ReLU       â”‚ max(0, x)                       â”‚ ÄÆ¡n giáº£n    â”‚
   â”‚ GELU       â”‚ x Â· Î¦(x)                        â”‚ Smooth      â”‚
   â”‚ SiLU/Swish â”‚ x Â· Ïƒ(x)                        â”‚ Smooth      â”‚
   â”‚ SwiGLU     â”‚ SiLU(Wx) âŠ™ (W'x)               â”‚ Gated, LLM  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")

# ============================================================
# INFERENCE DEMO
# ============================================================
print("=" * 60)
print("INFERENCE DEMO")
print("=" * 60)
prompt = "Äiá»n tá»« cÃ²n thiáº¿u vÃ o cÃ¢u \"HÃ´m nay trá»i ....\", tui Ä‘iá»n tá»« \"Ä‘áº¹p\" thÃ¬ cÃ³ Ä‘Ãºng ngá»¯ phÃ¡p vÃ  cáº¥u trÃºc cÃ¢u tiáº¿ng viá»‡t hay khÃ´ng?"
print(f"ğŸ“ Prompt: {prompt}")
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

with torch.no_grad():
    out = model.generate(
        **inputs,
        max_new_tokens=40,
        do_sample=False  # greedy cho á»•n Ä‘á»‹nh
    )

print(f"ğŸ¤– Output: {tokenizer.decode(out[0], skip_special_tokens=True)}")
